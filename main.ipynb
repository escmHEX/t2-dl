{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función de activación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(x, alpha=0.01):\n",
    "    return cp.maximum(alpha * x, x)\n",
    "\n",
    "def leaky_relu_derivative(x, alpha=0.01):\n",
    "    dx = cp.ones_like(x)\n",
    "    dx[x < 0] = alpha\n",
    "    return dx\n",
    "\n",
    "def softmax(z):#z: vector resultante antes de funcion de activación en la última capa\n",
    "    exp_z = cp.exp(z - cp.max(z, axis=1, keepdims=True))  \n",
    "    return exp_z / cp.sum(exp_z, axis=1, keepdims=True)#retorna las probabilidades de las posibles clases\n",
    "\n",
    "#mide la diferencia entre distribucion de prob. creada por softmax y los targets reales\n",
    "def cross_entropy_loss(predictions, targets):\n",
    "    return -cp.sum(targets * cp.log(predictions + 1e-9)) / targets.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clase MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiLayerNetwork:\n",
    "    def __init__(self, layer_sizes):\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "\n",
    "        # Initialize weights and biases for each layer\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            weight = cp.random.randn(layer_sizes[i], layer_sizes[i + 1]) * cp.sqrt(1. / layer_sizes[i])\n",
    "            bias = cp.zeros((1, layer_sizes[i + 1]))\n",
    "            self.weights.append(weight)\n",
    "            self.biases.append(bias)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.activations = [inputs]\n",
    "        a = inputs\n",
    "\n",
    "        for i in range(len(self.weights) - 1):\n",
    "            z = cp.dot(a, self.weights[i]) + self.biases[i]\n",
    "            a = leaky_relu(z)\n",
    "            self.activations.append(a)\n",
    "\n",
    "        # Output uses softmax for multiclass classification\n",
    "        z = cp.dot(a, self.weights[-1]) + self.biases[-1]\n",
    "        a = softmax(z)\n",
    "        self.activations.append(a)\n",
    "        return a\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        inputs = cp.array(inputs, ndmin=2)\n",
    "        a = inputs\n",
    "\n",
    "        for i in range(len(self.weights) - 1):\n",
    "            z = cp.dot(a, self.weights[i]) + self.biases[i]\n",
    "            a = leaky_relu(z)\n",
    "        \n",
    "        # Output with softmax\n",
    "        z = cp.dot(a, self.weights[-1]) + self.biases[-1]\n",
    "        a = softmax(z)\n",
    "\n",
    "        return a\n",
    "\n",
    "    def backward(self, targets, learning_rate, clip_value=None):\n",
    "        delta_weights = [0] * len(self.weights)\n",
    "        delta_biases = [0] * len(self.biases)\n",
    "        \n",
    "        # Calculate the initial error (difference between prediction and target for the output layer)\n",
    "        error = self.activations[-1] - targets\n",
    "        \n",
    "        for i in reversed(range(len(self.weights))):\n",
    "            # Calculate the delta for the current layer\n",
    "            delta = error\n",
    "            delta_weights[i] = cp.dot(self.activations[i].T, delta)\n",
    "            delta_biases[i] = cp.sum(delta, axis=0, keepdims=True)\n",
    "            \n",
    "            if i != 0:\n",
    "                # Propagate the error to the previous layer\n",
    "                error = cp.dot(delta, self.weights[i].T) * leaky_relu_derivative(self.activations[i])\n",
    "            \n",
    "            # Clip gradients if clip_value is provided\n",
    "            if clip_value:\n",
    "                delta_weights[i] = cp.clip(delta_weights[i], -clip_value, clip_value)\n",
    "                delta_biases[i] = cp.clip(delta_biases[i], -clip_value, clip_value)\n",
    "            \n",
    "            # Update weights and biases\n",
    "            self.weights[i] -= learning_rate * delta_weights[i]\n",
    "            self.biases[i] -= learning_rate * delta_biases[i]\n",
    "\n",
    "        #print(self.weights)\n",
    "        \n",
    "    def train(self, inputs, targets, epochs, learning_rate, clip_value=None):\n",
    "        inputs = cp.array(inputs, ndmin=2)\n",
    "        targets = cp.array(targets, ndmin=2)\n",
    "        errors = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            predictions = self.forward(inputs)\n",
    "            \n",
    "            self.backward(targets, learning_rate, clip_value)\n",
    "            error = cross_entropy_loss(predictions, targets)\n",
    "            errors.append(error)\n",
    "\n",
    "            print(predictions)\n",
    "            if epoch % 100 == 0:\n",
    "                print(f'Epoch {epoch}, Error: {error}')\n",
    "\n",
    "        return errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10155, 3074)\n",
      "(10155, 3072)\n",
      "(10155, 1)\n"
     ]
    }
   ],
   "source": [
    "#Obtener datos (estoy usando pd porque anda considerablemente más rápido que np)\n",
    "train_data = pd.read_csv(r'C:/Users/kueru/Documents/VSCode/semestre_9/Deep_Learning/T2/train_data_2.csv')\n",
    "train_data = train_data.to_numpy()\n",
    "    \n",
    "#Cortar en features y labales\n",
    "train_samples = train_data.shape[0]\n",
    "features = train_data[:train_samples, 1:-1]  # Features for training    \n",
    "labels = train_data[:train_samples, -1]  #Labels for training\n",
    "\n",
    "labels = labels.reshape(-1, 1)  # Reshape to (299, 1)\n",
    "\n",
    "X_train = cp.array(features)\n",
    "y_train = cp.array(labels, ndmin=2)\n",
    "\n",
    "print(train_data.shape)\n",
    "print(features.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00000000e+000 5.75118358e-023 7.46531849e-052 ... 1.40892032e-019\n",
      "  2.69626527e-055 6.72151028e-041]\n",
      " [1.00000000e+000 2.85011978e-054 9.72615225e-120 ... 3.12227626e-034\n",
      "  9.95388415e-103 8.76166278e-085]\n",
      " [1.00000000e+000 1.12142290e-028 3.70483863e-044 ... 6.39510007e-022\n",
      "  9.00183458e-055 4.10077246e-065]\n",
      " ...\n",
      " [1.00000000e+000 2.76471428e-036 3.62234706e-071 ... 2.95317926e-031\n",
      "  2.22041725e-059 3.75348821e-053]\n",
      " [1.00000000e+000 1.83134138e-042 3.38777212e-097 ... 2.11030662e-034\n",
      "  4.80360398e-084 4.70599808e-056]\n",
      " [1.00000000e+000 1.40646306e-030 2.43528675e-102 ... 3.03214506e-028\n",
      "  2.92160931e-081 2.00487026e-053]]\n",
      "Epoch 0, Error: 832.6109296796737\n",
      "[[1.00000000e+000 5.50772862e-020 1.14424230e-064 ... 1.52444924e-042\n",
      "  4.18841310e-069 8.00993398e-042]\n",
      " [1.00000000e+000 9.15861419e-025 5.40552882e-122 ... 7.87737902e-058\n",
      "  3.32138493e-105 3.61093994e-062]\n",
      " [9.90344520e-001 9.65547981e-003 1.29691487e-047 ... 8.55434514e-036\n",
      "  4.48351886e-055 1.70173543e-039]\n",
      " ...\n",
      " [1.00000000e+000 5.05207403e-011 1.84059203e-070 ... 7.34760144e-036\n",
      "  1.83863266e-078 3.67596998e-045]\n",
      " [1.00000000e+000 1.33486837e-033 5.38489084e-114 ... 7.08820234e-071\n",
      "  2.91934142e-113 3.03573897e-058]\n",
      " [1.00000000e+000 5.12444859e-035 1.28707052e-126 ... 6.73929199e-059\n",
      "  1.68659426e-109 3.81451133e-048]]\n",
      "[[1.00000000e+000 3.71137660e-024 1.03825961e-092 ... 1.95432399e-041\n",
      "  1.93093206e-111 3.56873125e-052]\n",
      " [1.00000000e+000 1.71715952e-028 2.42082342e-167 ... 9.09778328e-062\n",
      "  1.06899641e-201 8.81841993e-077]\n",
      " [9.99999988e-001 1.17478605e-008 6.81329248e-065 ... 2.98695319e-024\n",
      "  4.72031062e-099 1.22991213e-032]\n",
      " ...\n",
      " [1.00000000e+000 8.09816222e-017 1.46508748e-097 ... 7.94272938e-031\n",
      "  8.79351621e-127 1.75922208e-059]\n",
      " [1.00000000e+000 1.73091062e-022 4.47241358e-144 ... 2.17588900e-065\n",
      "  1.38409180e-171 2.75636767e-057]\n",
      " [1.00000000e+000 2.83156191e-027 7.74603775e-136 ... 1.79092351e-046\n",
      "  2.63043005e-148 1.66746312e-041]]\n",
      "[[1.00000000e+000 4.03860232e-018 2.31524396e-106 ... 1.41998380e-032\n",
      "  3.04262152e-153 1.05045876e-017]\n",
      " [1.00000000e+000 1.92214352e-015 2.68533695e-194 ... 8.52176521e-047\n",
      "  7.59301631e-289 4.11680804e-016]\n",
      " [1.43958694e-009 9.43356123e-010 7.67821899e-091 ... 2.50313854e-021\n",
      "  4.17978951e-144 9.99999987e-001]\n",
      " ...\n",
      " [3.78694342e-011 1.51739441e-005 1.98943194e-115 ... 2.74083129e-028\n",
      "  1.53622872e-185 7.46345834e-022]\n",
      " [9.98648888e-001 3.93654280e-008 1.25639698e-165 ... 7.85488253e-055\n",
      "  2.91075444e-243 1.35107277e-003]\n",
      " [5.82196730e-010 4.28688902e-013 5.68201468e-153 ... 1.87989045e-040\n",
      "  3.44931201e-191 9.99999999e-001]]\n",
      "[[9.99851358e-001 2.29769493e-024 7.58910806e-139 ... 7.46416095e-042\n",
      "  1.89321528e-223 2.41446314e-007]\n",
      " [8.54362386e-011 1.82695290e-038 6.91768871e-276 ... 3.73506892e-080\n",
      "  0.00000000e+000 9.29484753e-012]\n",
      " [1.11194333e-021 4.24969180e-026 1.33311060e-142 ... 7.44399540e-040\n",
      "  4.28463562e-228 1.72328292e-002]\n",
      " ...\n",
      " [1.22936971e-044 4.93529804e-028 1.59340763e-180 ... 1.86149864e-067\n",
      "  2.64440812e-307 8.99506481e-032]\n",
      " [1.25633495e-009 2.20084806e-026 4.46954501e-229 ... 3.71590057e-085\n",
      "  0.00000000e+000 5.86444119e-001]\n",
      " [1.38827542e-036 6.78451472e-037 4.41770357e-215 ... 1.98812214e-077\n",
      "  2.19056904e-288 1.00000000e+000]]\n",
      "[[1.00000000e+000 2.26628458e-042 6.87042795e-198 ... 2.04235959e-070\n",
      "  2.27832849e-313 1.00473006e-028]\n",
      " [1.00000000e+000 4.13748045e-073 0.00000000e+000 ... 5.19481496e-124\n",
      "  0.00000000e+000 2.05593229e-047]\n",
      " [9.51498317e-001 1.26457045e-028 1.53081212e-190 ... 5.06492395e-048\n",
      "  1.22669922e-308 6.90542785e-009]\n",
      " ...\n",
      " [5.04865591e-027 2.91033235e-035 4.11267305e-246 ... 4.95676034e-084\n",
      "  0.00000000e+000 5.23891905e-040]\n",
      " [1.00000000e+000 6.92334896e-051 1.22730896e-317 ... 4.23810565e-121\n",
      "  0.00000000e+000 2.01681854e-025]\n",
      " [2.01732031e-005 9.39375777e-033 4.90539559e-257 ... 8.19337573e-086\n",
      "  0.00000000e+000 9.99979827e-001]]\n",
      "[[1.00000000e+000 1.04935211e-042 1.61978503e-250 ... 1.05664239e-075\n",
      "  0.00000000e+000 6.96339060e-038]\n",
      " [1.00000000e+000 2.73061615e-072 0.00000000e+000 ... 2.66176162e-137\n",
      "  0.00000000e+000 3.34050046e-063]\n",
      " [9.99995824e-001 1.13169875e-026 2.79527847e-245 ... 1.02820745e-058\n",
      "  0.00000000e+000 3.28147889e-022]\n",
      " ...\n",
      " [1.36313795e-029 3.11012573e-031 7.85669820e-316 ... 7.07926469e-097\n",
      "  0.00000000e+000 1.47715342e-051]\n",
      " [1.00000000e+000 1.25693446e-053 0.00000000e+000 ... 1.60444685e-137\n",
      "  0.00000000e+000 2.76352838e-044]\n",
      " [1.00000000e+000 4.83388927e-032 1.45657674e-313 ... 2.18078501e-097\n",
      "  0.00000000e+000 3.23124726e-013]]\n",
      "[[1.00000000e+000 7.00690261e-048 3.74341552e-310 ... 3.42203603e-082\n",
      "  0.00000000e+000 5.41827436e-050]\n",
      " [1.00000000e+000 7.74503425e-087 0.00000000e+000 ... 3.08145613e-154\n",
      "  0.00000000e+000 5.71801284e-091]\n",
      " [1.00000000e+000 4.51928217e-048 2.56914136e-322 ... 1.84608017e-074\n",
      "  0.00000000e+000 9.53299025e-046]\n",
      " ...\n",
      " [8.48601711e-038 9.86384992e-048 0.00000000e+000 ... 3.42112998e-114\n",
      "  0.00000000e+000 2.69979711e-074]\n",
      " [1.00000000e+000 7.49238449e-069 0.00000000e+000 ... 6.13005494e-155\n",
      "  0.00000000e+000 2.34452987e-069]\n",
      " [1.00000000e+000 1.79667545e-054 0.00000000e+000 ... 7.40244256e-119\n",
      "  0.00000000e+000 1.91190330e-044]]\n",
      "[[1.00000000e+000 1.37503812e-044 0.00000000e+000 ... 2.58910808e-085\n",
      "  0.00000000e+000 2.65805626e-064]\n",
      " [1.00000000e+000 1.79379559e-091 0.00000000e+000 ... 8.76468388e-162\n",
      "  0.00000000e+000 4.93233821e-129]\n",
      " [1.00000000e+000 2.99707758e-052 0.00000000e+000 ... 2.67520809e-079\n",
      "  0.00000000e+000 2.68280862e-066]\n",
      " ...\n",
      " [4.79877088e-034 4.88947790e-044 0.00000000e+000 ... 9.58408586e-116\n",
      "  0.00000000e+000 4.52443489e-095]\n",
      " [1.00000000e+000 7.30017592e-068 0.00000000e+000 ... 2.81813177e-161\n",
      "  0.00000000e+000 4.37036546e-093]\n",
      " [1.00000000e+000 8.63444293e-054 0.00000000e+000 ... 3.43053450e-122\n",
      "  0.00000000e+000 4.59761296e-062]]\n",
      "[[9.99978683e-001 5.59051036e-034 0.00000000e+000 ... 4.35092077e-093\n",
      "  0.00000000e+000 2.76457918e-077]\n",
      " [1.00000000e+000 1.05109955e-074 0.00000000e+000 ... 3.91546810e-179\n",
      "  0.00000000e+000 1.68650547e-157]\n",
      " [1.00000000e+000 5.34597661e-046 0.00000000e+000 ... 6.63440930e-088\n",
      "  0.00000000e+000 3.56396678e-080]\n",
      " ...\n",
      " [5.24363199e-035 1.83196821e-035 0.00000000e+000 ... 1.27039499e-132\n",
      "  0.00000000e+000 3.53722420e-120]\n",
      " [1.00000000e+000 1.40644128e-048 0.00000000e+000 ... 1.50744849e-172\n",
      "  0.00000000e+000 6.08236735e-113]\n",
      " [1.00000000e+000 3.45272715e-041 0.00000000e+000 ... 8.43407266e-128\n",
      "  0.00000000e+000 4.28373852e-078]]\n",
      "[[2.37774267e-003 1.37101669e-025 0.00000000e+000 ... 2.27985582e-107\n",
      "  0.00000000e+000 1.69183767e-093]\n",
      " [9.99999938e-001 6.11754884e-053 0.00000000e+000 ... 1.93267825e-202\n",
      "  0.00000000e+000 3.88579655e-184]\n",
      " [9.13181154e-001 1.38396216e-032 0.00000000e+000 ... 4.54434808e-101\n",
      "  0.00000000e+000 2.49042619e-094]\n",
      " ...\n",
      " [2.13353397e-036 3.59451773e-027 0.00000000e+000 ... 4.10010037e-158\n",
      "  0.00000000e+000 4.87891305e-144]\n",
      " [3.07543330e-002 1.87002407e-029 0.00000000e+000 ... 2.83965768e-190\n",
      "  0.00000000e+000 5.72660709e-136]\n",
      " [1.00000000e+000 1.94069077e-024 0.00000000e+000 ... 2.11441190e-140\n",
      "  0.00000000e+000 4.26410621e-091]]\n",
      "[[7.54737657e-001 1.80680920e-011 0.00000000e+000 ... 7.28255073e-126\n",
      "  0.00000000e+000 1.88402790e-109]\n",
      " [9.99998920e-001 1.39982197e-028 0.00000000e+000 ... 4.47570844e-246\n",
      "  0.00000000e+000 7.58161378e-216]\n",
      " [6.99192769e-005 2.73544717e-017 0.00000000e+000 ... 6.12187671e-125\n",
      "  0.00000000e+000 3.25845219e-113]\n",
      " ...\n",
      " [4.10953703e-028 1.55239778e-006 0.00000000e+000 ... 1.94193471e-181\n",
      "  0.00000000e+000 7.32060992e-159]\n",
      " [9.63171544e-001 4.11205546e-007 0.00000000e+000 ... 4.43490999e-219\n",
      "  0.00000000e+000 8.34860632e-163]\n",
      " [1.00000000e+000 2.38664282e-012 0.00000000e+000 ... 1.92733619e-163\n",
      "  0.00000000e+000 3.82765482e-112]]\n",
      "[[9.99956059e-001 2.13862291e-008 0.00000000e+000 ... 5.99576472e-154\n",
      "  0.00000000e+000 1.99672487e-134]\n",
      " [1.00000000e+000 5.35759469e-025 0.00000000e+000 ... 2.74494996e-304\n",
      "  0.00000000e+000 8.09690557e-264]\n",
      " [1.22130368e-003 1.15817435e-011 0.00000000e+000 ... 3.53020474e-152\n",
      "  0.00000000e+000 2.99175168e-135]\n",
      " ...\n",
      " [1.43379061e-026 6.48270400e-002 0.00000000e+000 ... 6.31197715e-219\n",
      "  0.00000000e+000 9.53539023e-184]\n",
      " [9.29959502e-001 7.00404667e-002 0.00000000e+000 ... 5.16253613e-262\n",
      "  0.00000000e+000 6.74504277e-203]\n",
      " [1.00000000e+000 3.29547122e-010 0.00000000e+000 ... 6.12197505e-192\n",
      "  0.00000000e+000 8.22654444e-141]]\n",
      "[[9.99999993e-001 1.29216433e-010 0.00000000e+000 ... 7.65073666e-193\n",
      "  0.00000000e+000 3.68991287e-154]\n",
      " [1.00000000e+000 2.25112338e-032 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 6.17615297e-306]\n",
      " [1.22357516e-002 5.51364854e-013 0.00000000e+000 ... 1.72907871e-192\n",
      "  0.00000000e+000 1.47654414e-155]\n",
      " ...\n",
      " [5.91117160e-022 1.03065192e-001 0.00000000e+000 ... 4.75500955e-268\n",
      "  0.00000000e+000 6.68548895e-205]\n",
      " [1.00000000e+000 2.44708693e-010 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 8.72794801e-244]\n",
      " [1.00000000e+000 3.41383936e-014 0.00000000e+000 ... 9.50239276e-234\n",
      "  0.00000000e+000 3.56076083e-169]]\n",
      "[[9.99999996e-001 6.00143915e-012 0.00000000e+000 ... 1.80959573e-232\n",
      "  0.00000000e+000 5.59142462e-178]\n",
      " [1.00000000e+000 1.06700854e-040 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]\n",
      " [1.22751603e-003 2.13203087e-015 0.00000000e+000 ... 9.64373406e-237\n",
      "  0.00000000e+000 4.67401138e-183]\n",
      " ...\n",
      " [2.15536223e-015 9.82419106e-001 0.00000000e+000 ... 2.57640412e-319\n",
      "  0.00000000e+000 4.84437675e-233]\n",
      " [1.00000000e+000 3.19441396e-018 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 1.08773030e-285]\n",
      " [1.00000000e+000 1.21008146e-012 0.00000000e+000 ... 5.70580682e-281\n",
      "  0.00000000e+000 2.38306958e-194]]\n",
      "[[9.99999759e-001 2.25910005e-007 0.00000000e+000 ... 1.21840052e-271\n",
      "  0.00000000e+000 3.39786643e-200]\n",
      " [1.00000000e+000 1.06288873e-032 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]\n",
      " [1.90041028e-003 6.21134615e-010 0.00000000e+000 ... 1.62686829e-280\n",
      "  0.00000000e+000 7.24113047e-209]\n",
      " ...\n",
      " [2.15799710e-018 1.00000000e+000 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 6.93239820e-271]\n",
      " [1.00000000e+000 8.28287803e-013 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]\n",
      " [9.99964045e-001 3.59552926e-005 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 7.07841572e-222]]\n",
      "[[1.00000000e+000 4.44225336e-017 0.00000000e+000 ... 2.70896736e-315\n",
      "  0.00000000e+000 5.60546530e-236]\n",
      " [1.00000000e+000 7.43670509e-052 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]\n",
      " [9.99999788e-001 5.26496442e-017 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 1.93761575e-246]\n",
      " ...\n",
      " [1.16371411e-006 9.99998836e-001 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 5.55990290e-308]\n",
      " [1.00000000e+000 2.13635647e-030 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]\n",
      " [1.00000000e+000 3.42326709e-016 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 7.80263210e-269]]\n",
      "[[1.00000000e+000 6.84645221e-020 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 3.82011060e-285]\n",
      " [1.00000000e+000 2.17995227e-060 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]\n",
      " [1.00000000e+000 7.37946264e-018 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 2.53297403e-297]\n",
      " ...\n",
      " [6.60273692e-001 3.39726308e-001 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]\n",
      " [1.00000000e+000 9.12085583e-037 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]\n",
      " [1.00000000e+000 3.14350274e-018 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]]\n",
      "[[1.00000000e+00 3.61944213e-25 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 2.25150628e-71 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 1.21221683e-20 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " ...\n",
      " [9.99999960e-01 3.97829963e-08 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 5.69494198e-46 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.65614006e-24 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]]\n",
      "[[1.00000000e+00 3.53828018e-32 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.04716043e-87 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 9.05256101e-26 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " ...\n",
      " [1.00000000e+00 8.43126297e-18 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.40959197e-57 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 1.73371492e-32 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]]\n",
      "[[1.00000000e+000 1.48610814e-044 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]\n",
      " [1.00000000e+000 9.84431689e-112 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]\n",
      " [1.00000000e+000 5.47848588e-039 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]\n",
      " ...\n",
      " [1.00000000e+000 4.52972500e-033 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]\n",
      " [1.00000000e+000 2.57199879e-076 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]\n",
      " [1.00000000e+000 1.11464122e-046 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]]\n",
      "[[1.00000000e+000 6.82579987e-068 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]\n",
      " [1.00000000e+000 3.33695259e-160 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]\n",
      " [1.00000000e+000 2.42677396e-065 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]\n",
      " ...\n",
      " [1.00000000e+000 3.50760929e-065 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]\n",
      " [1.00000000e+000 2.55548352e-114 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]\n",
      " [1.00000000e+000 1.99695215e-076 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]]\n",
      "[[1.00000000e+000 4.12408850e-097 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]\n",
      " [1.00000000e+000 3.84302175e-218 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]\n",
      " [1.00000000e+000 1.06455207e-096 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]\n",
      " ...\n",
      " [1.00000000e+000 1.16876022e-104 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]\n",
      " [1.00000000e+000 8.08773900e-162 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]\n",
      " [1.00000000e+000 3.00217681e-110 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]]\n",
      "[[1.00000000e+000 1.73071974e-131 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]\n",
      " [1.00000000e+000 9.16843013e-287 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]\n",
      " [1.00000000e+000 1.56718256e-133 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]\n",
      " ...\n",
      " [1.00000000e+000 8.52895305e-152 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]\n",
      " [1.00000000e+000 4.86531981e-221 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]\n",
      " [1.00000000e+000 4.11021161e-154 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]]\n",
      "[[1.00000000e+000 7.41351154e-175 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]\n",
      " [1.00000000e+000 0.00000000e+000 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]\n",
      " [1.00000000e+000 1.69053666e-180 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]\n",
      " ...\n",
      " [1.00000000e+000 1.27103359e-207 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]\n",
      " [1.00000000e+000 5.55038997e-294 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]\n",
      " [1.00000000e+000 8.67813557e-211 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]]\n",
      "[[1.00000000e+000 8.26025191e-227 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]\n",
      " [1.00000000e+000 0.00000000e+000 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]\n",
      " [1.00000000e+000 1.27159256e-234 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]\n",
      " ...\n",
      " [1.00000000e+000 2.81699450e-269 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]\n",
      " [1.00000000e+000 0.00000000e+000 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]\n",
      " [1.00000000e+000 6.09067872e-277 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]]\n",
      "[[1.00000000e+000 2.61999197e-286 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]\n",
      " [1.00000000e+000 0.00000000e+000 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]\n",
      " [1.00000000e+000 1.21551271e-302 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]\n",
      " ...\n",
      " [1.00000000e+000 0.00000000e+000 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]\n",
      " [1.00000000e+000 0.00000000e+000 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]\n",
      " [1.00000000e+000 0.00000000e+000 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m      9\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0001\u001b[39m\n\u001b[1;32m---> 11\u001b[0m errors \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2.5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#Evaluar\u001b[39;00m\n\u001b[0;32m     13\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_train)\n",
      "Cell \u001b[1;32mIn[3], line 83\u001b[0m, in \u001b[0;36mMultiLayerNetwork.train\u001b[1;34m(self, inputs, targets, epochs, learning_rate, clip_value)\u001b[0m\n\u001b[0;32m     80\u001b[0m error \u001b[38;5;241m=\u001b[39m cross_entropy_loss(predictions, targets)\n\u001b[0;32m     81\u001b[0m errors\u001b[38;5;241m.\u001b[39mappend(error)\n\u001b[1;32m---> 83\u001b[0m \u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mcupy\\_core\\core.pyx:1741\u001b[0m, in \u001b[0;36mcupy._core.core._ndarray_base.__str__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "input_size = 3072\n",
    "hidden_layers = [256,256,256]  # Tamaños de las capas ocultas\n",
    "output_size = 10\n",
    "layer_sizes = [input_size] + hidden_layers + [output_size]\n",
    "\n",
    "model = MultiLayerNetwork(layer_sizes)\n",
    "epochs = 100\n",
    "learning_rate = 0.0001\n",
    "\n",
    "errors = model.train(X_train, y_train, epochs, learning_rate, 2.5)\n",
    "#Evaluar\n",
    "predictions = model.predict(X_train)\n",
    "accuracy = cp.mean(predictions.argmax(axis=1) == y_train)\n",
    "print(f'Test Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Búsqueda de parámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Searh para buscar learning rate:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate de: 1e-14\n",
      "Epoch 0, Error: 120.66645697607764\n",
      "Test Accuracy: 0.07394771870560732\n",
      "\n",
      "Learning rate de: 1e-05\n",
      "Epoch 0, Error: 106.33975866854324\n",
      "Test Accuracy: 0.14046822742474915\n",
      "\n",
      "Learning rate de: 0.0001\n",
      "Epoch 0, Error: 107.23506685699824\n",
      "Test Accuracy: 0.14046822742474915\n",
      "\n",
      "Learning rate de: 0.001\n",
      "Epoch 0, Error: 135.5762837276628\n",
      "Test Accuracy: 0.14046822742474915\n",
      "\n",
      "Learning rate de: 0.01\n",
      "Epoch 0, Error: 107.94783862127046\n",
      "Test Accuracy: 0.14046822742474915\n",
      "\n",
      "Learning rate de: 0.1\n",
      "Epoch 0, Error: 121.5481609965781\n",
      "Test Accuracy: 0.14046822742474915\n",
      "\n",
      "Learning rate de: 1\n",
      "Epoch 0, Error: 111.2974164633621\n",
      "Test Accuracy: 0.14046822742474915\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "epochs = 100\n",
    "learning_rate = [10e-15,0.00001, 0.0001,0.001, 0.01, 0.1, 1]\n",
    "\n",
    "for lr in learning_rate:\n",
    "    print(f\"Learning rate de: {lr}\")\n",
    "\n",
    "    model = MultiLayerNetwork(layer_sizes)\n",
    "    errors = model.train(X_train, y_train, epochs, lr)\n",
    "\n",
    "    #Evaluar\n",
    "    predictions = model.predict(X_train)\n",
    "    accuracy = cp.mean(predictions.argmax(axis=1) == y_train)\n",
    "    print(f'Test Accuracy: {accuracy}')\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relación entre learning rate y número de épocas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate de: 1e-14\n",
      "Epochs: 100\n",
      "Test Accuracy: 0.09089383787653382\n",
      "\n",
      "Learning rate de: 1e-14\n",
      "Epochs: 500\n",
      "Test Accuracy: 0.09392512388004609\n",
      "\n",
      "Learning rate de: 1e-14\n",
      "Epochs: 1000\n",
      "Test Accuracy: 0.11373474569635686\n",
      "\n",
      "Learning rate de: 1e-14\n",
      "Epochs: 2500\n",
      "Test Accuracy: 0.1080748537488395\n",
      "\n",
      "Learning rate de: 1e-14\n",
      "Epochs: 5000\n",
      "Test Accuracy: 0.07074864934396707\n",
      "\n",
      "Learning rate de: 0.01\n",
      "Epochs: 100\n",
      "Test Accuracy: 0.14046822742474915\n",
      "\n",
      "Learning rate de: 0.01\n",
      "Epochs: 500\n",
      "Test Accuracy: 0.14046822742474915\n",
      "\n",
      "Learning rate de: 0.01\n",
      "Epochs: 1000\n",
      "Test Accuracy: 0.14046822742474915\n",
      "\n",
      "Learning rate de: 0.01\n",
      "Epochs: 2500\n",
      "Test Accuracy: 0.14046822742474915\n",
      "\n",
      "Learning rate de: 0.01\n",
      "Epochs: 5000\n",
      "Test Accuracy: 0.14046822742474915\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_classes = 10\n",
    "input_size = 3072\n",
    "hidden_layers = [256]  # Tamaños de las capas ocultas\n",
    "output_size = 10\n",
    "layer_sizes = [input_size] + hidden_layers + [output_size]\n",
    "\n",
    "epochs = [100, 500, 1000, 2500, 5000]\n",
    "learning_rate = [10e-15, 0.01]\n",
    "lr_e_results = []\n",
    "for lr in learning_rate:\n",
    "    for e in epochs:\n",
    "        print(f\"Learning rate de: {lr}\")\n",
    "        print(f\"Epochs: {e}\")\n",
    "\n",
    "        model = MultiLayerNetwork(layer_sizes)\n",
    "        errors = model.train(X_train, y_train, e, lr)\n",
    "\n",
    "        #Evaluar\n",
    "        predictions = model.predict(X_train)\n",
    "        accuracy = cp.mean(predictions.argmax(axis=1) == y_train)\n",
    "        print(f'Test Accuracy: {accuracy}')\n",
    "        print(\"\")\n",
    "        lr_e_results.append(accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Searh para ajustar complejitud de 1 capa oculta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer size: 1\n",
      "[3072, 1, 10]\n",
      "Epoch 0, Error: 102.89002775587444\n",
      "Test Accuracy: 0.11705685618729098\n",
      "\n",
      "layer size: 10\n",
      "[3072, 10, 10]\n",
      "Epoch 0, Error: 107.33877121476301\n",
      "Test Accuracy: 0.14046822742474915\n",
      "\n",
      "layer size: 100\n",
      "[3072, 100, 10]\n",
      "Epoch 0, Error: 105.03349595272618\n",
      "Test Accuracy: 0.0903010033444816\n",
      "\n",
      "layer size: 1000\n",
      "[3072, 1000, 10]\n",
      "Epoch 0, Error: 112.76559406381097\n",
      "Test Accuracy: 0.14046822742474915\n",
      "\n",
      "layer size: 10000\n",
      "[3072, 10000, 10]\n",
      "Epoch 0, Error: 116.52473388148074\n",
      "Test Accuracy: 0.0903010033444816\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "epochs = 100\n",
    "layer_sizes = [input_size] + hidden_layers + [output_size]\n",
    "lr = 0.00001\n",
    "for i in range(0, 5):  # Cambia el 5 por el número de potencias de 10 que desees\n",
    "    val = pow(10, i)\n",
    "    hidden_layers = [val]  # Tamaños de las capas ocultas\n",
    "    print(f\"layer size: {val}\")\n",
    "    layer_sizes = [input_size] + hidden_layers + [output_size]\n",
    "    print(layer_sizes)\n",
    "\n",
    "    model = MultiLayerNetwork(layer_sizes)\n",
    "    errors = model.train(X_train, y_train, epochs, lr)\n",
    "\n",
    "    #Evaluar\n",
    "    predictions = model.predict(X_train)\n",
    "    accuracy = cp.mean(predictions.argmax(axis=1) == y_train)\n",
    "    print(f'Test Accuracy: {accuracy}')\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search para ajustar numero de capas ocultas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden layers: [256]\n",
      "Epoch 0, Error: 105.86025886841283\n",
      "Test Accuracy: 0.09698996655518395\n",
      "\n",
      "hidden layers: [256, 256]\n",
      "Epoch 0, Error: 108.85456644116445\n",
      "Test Accuracy: 0.14046822742474915\n",
      "\n",
      "hidden layers: [256, 256, 256]\n",
      "Epoch 0, Error: 105.7057006341719\n",
      "Test Accuracy: 0.14046822742474915\n",
      "\n",
      "hidden layers: [256, 256, 256, 256]\n",
      "Epoch 0, Error: 110.30727016766471\n",
      "Test Accuracy: 0.14046822742474915\n",
      "\n",
      "hidden layers: [256, 256, 256, 256, 256]\n",
      "Epoch 0, Error: 125.7849884262953\n",
      "Test Accuracy: 0.14046822742474915\n",
      "\n",
      "hidden layers: [256, 256, 256, 256, 256, 256]\n",
      "Epoch 0, Error: 111.10845764177878\n",
      "Test Accuracy: 0.14046822742474915\n",
      "\n",
      "hidden layers: [256, 256, 256, 256, 256, 256, 256]\n",
      "Epoch 0, Error: 106.9112295635407\n",
      "Test Accuracy: 0.14046822742474915\n",
      "\n",
      "hidden layers: [256, 256, 256, 256, 256, 256, 256, 256]\n",
      "Epoch 0, Error: 107.25215446902249\n",
      "Test Accuracy: 0.14046822742474915\n",
      "\n",
      "hidden layers: [256, 256, 256, 256, 256, 256, 256, 256, 256]\n",
      "Epoch 0, Error: 105.26643352242563\n",
      "Test Accuracy: 0.14046822742474915\n",
      "\n",
      "hidden layers: [256, 256, 256, 256, 256, 256, 256, 256, 256, 256]\n",
      "Epoch 0, Error: 120.37974780141502\n",
      "Test Accuracy: 0.14046822742474915\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_classes = 10\n",
    "input_size = 3072\n",
    "hidden_layer_size = 256\n",
    "hidden_layers = []  # Tamaños de las capas ocultas\n",
    "output_size = 10\n",
    "layer_sizes = [input_size] + hidden_layers + [output_size]\n",
    "epochs = 100\n",
    "lr = 0.0001\n",
    "\n",
    "for i in range(0, 10):  # Cambia el 5 por el número de potencias de 10 que desees\n",
    "    hidden_layers.append(hidden_layer_size)  # Tamaños de las capas ocultas\n",
    "    print(f\"hidden layers: {hidden_layers}\")\n",
    "    layer_sizes = [input_size] + hidden_layers + [output_size]\n",
    "\n",
    "    model = MultiLayerNetwork(layer_sizes)\n",
    "    errors = model.train(X_train, y_train, epochs, lr)\n",
    "\n",
    "    #Evaluar\n",
    "    predictions = model.predict(X_train)\n",
    "    accuracy = cp.mean(predictions.argmax(axis=1) == y_train)\n",
    "    print(f'Test Accuracy: {accuracy}')\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo entrenado con parámetros encontrados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(107.19914646), array(691.01418078), array(810.28662508), array(810.28662508), array(810.28662508), array(252.3364584), array(726.27750819), array(252.3364584), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan), array(nan)]\n",
      "Test Accuracy: 0.14046822742474915\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "input_size = 3072\n",
    "hidden_layers = [10, 10]  # Tamaños de las capas ocultas\n",
    "output_size = 10\n",
    "layer_sizes = [input_size] + hidden_layers + [output_size]\n",
    "epochs = 500\n",
    "learning_rate = 0.0001\n",
    "\n",
    "model = MultiLayerNetwork(layer_sizes)\n",
    "errors = model.train(X_train, y_train, epochs, learning_rate)\n",
    "print(errors)\n",
    "#Evaluar\n",
    "predictions = model.predict(X_train)\n",
    "accuracy = cp.mean(predictions.argmax(axis=1) == y_train)\n",
    "print(f'Test Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularizaciones:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiLayerNetwork_Dropout:\n",
    "    def __init__(self, layer_sizes, dropout_rate):\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # Initialize weights and biases for each layer\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            weight = cp.random.randn(layer_sizes[i], layer_sizes[i + 1]) * cp.sqrt(2. / layer_sizes[i])\n",
    "            bias = cp.zeros((1, layer_sizes[i + 1]))\n",
    "            self.weights.append(weight)\n",
    "            self.biases.append(bias)\n",
    "\n",
    "    def forward(self, inputs, training=True):\n",
    "        self.activations = [inputs]\n",
    "        a = inputs\n",
    "\n",
    "        for i in range(len(self.weights) - 1):\n",
    "            z = cp.dot(a, self.weights[i]) + self.biases[i]\n",
    "            a = relu(z)\n",
    "            # Apply dropout during training\n",
    "            if training:\n",
    "                dropout_mask = cp.random.binomial(1, 1 - self.dropout_rate, size=a.shape) / (1 - self.dropout_rate)\n",
    "                a *= dropout_mask\n",
    "            self.activations.append(a)\n",
    "        \n",
    "        # Output with softmax\n",
    "        z = cp.dot(a, self.weights[-1]) + self.biases[-1]\n",
    "        a = softmax(z)\n",
    "        self.activations.append(a)\n",
    "\n",
    "        return a\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        inputs = cp.array(inputs, ndmin=2)\n",
    "        a = inputs\n",
    "\n",
    "        for i in range(len(self.weights) - 1):\n",
    "            z = cp.dot(a, self.weights[i]) + self.biases[i]\n",
    "            a = relu(z)\n",
    "        \n",
    "        # Output with softmax\n",
    "        z = cp.dot(a, self.weights[-1]) + self.biases[-1]\n",
    "        a = softmax(z)\n",
    "\n",
    "        return a\n",
    "\n",
    "    def backward(self, targets, learning_rate):\n",
    "        delta_weights = [0] * len(self.weights)\n",
    "        delta_biases = [0] * len(self.biases)\n",
    "        \n",
    "        # Calculate the initial error (difference between prediction and target for the output layer)\n",
    "        error = self.activations[-1] - targets\n",
    "        \n",
    "        for i in reversed(range(len(self.weights))):\n",
    "            # Calculate the delta for the current layer\n",
    "            delta = error\n",
    "            delta_weights[i] = cp.dot(self.activations[i].T, delta)\n",
    "            delta_biases[i] = cp.sum(delta, axis=0, keepdims=True)\n",
    "            \n",
    "            if i != 0:\n",
    "                # Propagate the error to the previous layer\n",
    "                error = cp.dot(delta, self.weights[i].T) * relu_derivative(self.activations[i])\n",
    "            \n",
    "            # Update weights and biases\n",
    "            self.weights[i] -= learning_rate * delta_weights[i]\n",
    "            self.biases[i] -= learning_rate * delta_biases[i]\n",
    "\n",
    "\n",
    "    def train(self, inputs, targets, epochs, learning_rate):\n",
    "        inputs = cp.array(inputs, ndmin=2)\n",
    "        targets = cp.array(targets, ndmin=2)\n",
    "        errors = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            predictions = self.forward(inputs, training=True)\n",
    "            self.backward(targets, learning_rate)\n",
    "            error = cross_entropy_loss(predictions, targets)\n",
    "            errors.append(error)\n",
    "\n",
    "            #if epoch % 100 == 0:\n",
    "            #    print(f'Epoch {epoch}, Error: {error}')\n",
    "\n",
    "        return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "CUDARuntimeError",
     "evalue": "cudaErrorLaunchTimeout: the launch timed out and was terminated",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCUDARuntimeError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m dropout_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m epochs:\n\u001b[1;32m---> 11\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mMultiLayerNetwork_Dropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     errors \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtrain(X_train, y_train, e, learning_rate)\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m#Evaluar\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[12], line 10\u001b[0m, in \u001b[0;36mMultiLayerNetwork_Dropout.__init__\u001b[1;34m(self, layer_sizes, dropout_rate)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Initialize weights and biases for each layer\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(layer_sizes) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 10\u001b[0m     weight \u001b[38;5;241m=\u001b[39m \u001b[43mcp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer_sizes\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_sizes\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m cp\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m2.\u001b[39m \u001b[38;5;241m/\u001b[39m layer_sizes[i])\n\u001b[0;32m     11\u001b[0m     bias \u001b[38;5;241m=\u001b[39m cp\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m, layer_sizes[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights\u001b[38;5;241m.\u001b[39mappend(weight)\n",
      "File \u001b[1;32mc:\\Users\\kueru\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\cupy\\random\\_sample.py:84\u001b[0m, in \u001b[0;36mrandn\u001b[1;34m(*size, **kwarg)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwarg:\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandn() got unexpected keyword arguments \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     83\u001b[0m                     \u001b[38;5;241m%\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(kwarg\u001b[38;5;241m.\u001b[39mkeys()))\n\u001b[1;32m---> 84\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_distributions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormal\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kueru\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\cupy\\random\\_distributions.py:501\u001b[0m, in \u001b[0;36mnormal\u001b[1;34m(loc, scale, size, dtype)\u001b[0m\n\u001b[0;32m    483\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns an array of normally distributed samples.\u001b[39;00m\n\u001b[0;32m    484\u001b[0m \n\u001b[0;32m    485\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    498\u001b[0m \n\u001b[0;32m    499\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    500\u001b[0m rs \u001b[38;5;241m=\u001b[39m _generator\u001b[38;5;241m.\u001b[39mget_random_state()\n\u001b[1;32m--> 501\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kueru\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\cupy\\random\\_generator.py:471\u001b[0m, in \u001b[0;36mRandomState.normal\u001b[1;34m(self, loc, scale, size, dtype)\u001b[0m\n\u001b[0;32m    469\u001b[0m     cupy\u001b[38;5;241m.\u001b[39madd(x, loc, out\u001b[38;5;241m=\u001b[39mx)\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 471\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_normal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    472\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\kueru\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\cupy\\random\\_generator.py:85\u001b[0m, in \u001b[0;36mRandomState._generate_normal\u001b[1;34m(self, func, size, dtype, *args)\u001b[0m\n\u001b[0;32m     83\u001b[0m element_size \u001b[38;5;241m=\u001b[39m _core\u001b[38;5;241m.\u001b[39minternal\u001b[38;5;241m.\u001b[39mprod(size)\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m element_size \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 85\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcupy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m     func(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generator, out\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mptr, out\u001b[38;5;241m.\u001b[39msize, \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\kueru\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\cupy\\_creation\\basic.py:31\u001b[0m, in \u001b[0;36mempty\u001b[1;34m(shape, dtype, order)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mempty\u001b[39m(\n\u001b[0;32m     13\u001b[0m         shape: _ShapeLike,\n\u001b[0;32m     14\u001b[0m         dtype: DTypeLike \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m,\n\u001b[0;32m     15\u001b[0m         order: _OrderCF \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     16\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NDArray[Any]:\n\u001b[0;32m     17\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns an array without initializing the elements.\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m \n\u001b[0;32m     30\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcupy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mndarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mcupy\\_core\\core.pyx:132\u001b[0m, in \u001b[0;36mcupy._core.core.ndarray.__new__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mcupy\\_core\\core.pyx:220\u001b[0m, in \u001b[0;36mcupy._core.core._ndarray_base._init\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mcupy\\cuda\\memory.pyx:738\u001b[0m, in \u001b[0;36mcupy.cuda.memory.alloc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mcupy\\cuda\\memory.pyx:1424\u001b[0m, in \u001b[0;36mcupy.cuda.memory.MemoryPool.malloc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mcupy\\cuda\\memory.pyx:1445\u001b[0m, in \u001b[0;36mcupy.cuda.memory.MemoryPool.malloc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mcupy\\cuda\\memory.pyx:1116\u001b[0m, in \u001b[0;36mcupy.cuda.memory.SingleDeviceMemoryPool.malloc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mcupy\\cuda\\memory.pyx:1137\u001b[0m, in \u001b[0;36mcupy.cuda.memory.SingleDeviceMemoryPool._malloc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mcupy\\cuda\\memory.pyx:1365\u001b[0m, in \u001b[0;36mcupy.cuda.memory.SingleDeviceMemoryPool._try_malloc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mcupy\\cuda\\memory.pyx:1362\u001b[0m, in \u001b[0;36mcupy.cuda.memory.SingleDeviceMemoryPool._try_malloc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mcupy\\cuda\\memory.pyx:1087\u001b[0m, in \u001b[0;36mcupy.cuda.memory.SingleDeviceMemoryPool._alloc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mcupy\\cuda\\memory.pyx:633\u001b[0m, in \u001b[0;36mcupy.cuda.memory._malloc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mcupy\\cuda\\memory.pyx:634\u001b[0m, in \u001b[0;36mcupy.cuda.memory._malloc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mcupy\\cuda\\memory.pyx:101\u001b[0m, in \u001b[0;36mcupy.cuda.memory.Memory.__init__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mcupy_backends\\cuda\\api\\runtime.pyx:498\u001b[0m, in \u001b[0;36mcupy_backends.cuda.api.runtime.malloc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mcupy_backends\\cuda\\api\\runtime.pyx:146\u001b[0m, in \u001b[0;36mcupy_backends.cuda.api.runtime.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mCUDARuntimeError\u001b[0m: cudaErrorLaunchTimeout: the launch timed out and was terminated"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "input_size = 3072\n",
    "hidden_layers = [256,256,256,256]  # Tamaños de las capas ocultas\n",
    "output_size = 10\n",
    "layer_sizes = [input_size] + hidden_layers + [output_size]\n",
    "epochs = [1, 5, 10, 100, 500, 1000]\n",
    "learning_rate = 0.00001\n",
    "dropout_rate = 0.2\n",
    "\n",
    "for e in epochs:\n",
    "    model = MultiLayerNetwork_Dropout(layer_sizes, dropout_rate)\n",
    "    errors = model.train(X_train, y_train, e, learning_rate)\n",
    "    #Evaluar\n",
    "    predictions = model.predict(X_train)\n",
    "    accuracy = cp.mean(predictions.argmax(axis=1) == y_train)\n",
    "    print(f\"epochs: {e}\")\n",
    "    print(f'Test Accuracy: {accuracy}')\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No mejora nada k chucha ( )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Porcentaje para cada canal RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_pixels = []\n",
    "green_pixels = []\n",
    "blue_pixels = []\n",
    "train_data = train_data[:, 1:-1]\n",
    "\n",
    "# Iterate through each row in the original matrix\n",
    "for row in train_data:\n",
    "    pr = []\n",
    "    pg = []\n",
    "    pb = []\n",
    "    for i in range(len(row)):\n",
    "        if i % 3 == 0:\n",
    "            pr.append(row[i])\n",
    "        \n",
    "        if i % 3 == 1:\n",
    "            pg.append(row[i])\n",
    "\n",
    "        if i % 3 == 2:\n",
    "            pb.append(row[i])\n",
    "\n",
    "    red_pixels.append(pr)\n",
    "    green_pixels.append(pg)\n",
    "    blue_pixels.append(pb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Más datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10155, 12)\n"
     ]
    }
   ],
   "source": [
    "#intensidad media por conal\n",
    "mean_red = np.mean(red_pixels, axis=1)\n",
    "mean_green = np.mean(green_pixels, axis=1)\n",
    "mean_blue = np.mean(blue_pixels, axis=1)\n",
    "\n",
    "#varianza\n",
    "var_red = np.var(red_pixels, axis=1)\n",
    "var_green = np.var(green_pixels, axis=1)\n",
    "var_blue = np.var(blue_pixels, axis=1)\n",
    "\n",
    "#desviación estándar\n",
    "std_red = np.std(red_pixels, axis=1)\n",
    "std_green = np.std(green_pixels, axis=1)\n",
    "std_blue = np.std(blue_pixels, axis=1)\n",
    "\n",
    "#contraste\n",
    "contrast_red = np.ptp(red_pixels, axis=1) # ptp: peak to peak (max - min)\n",
    "contrast_green = np.ptp(green_pixels, axis=1)\n",
    "contrast_blue = np.ptp(blue_pixels, axis=1)\n",
    "\n",
    "channel_data = np.column_stack((\n",
    "                                mean_red, mean_green, mean_blue,\n",
    "                                var_red, var_green, var_blue,\n",
    "                                std_red, std_green, std_blue,\n",
    "                                contrast_red, contrast_green, contrast_blue\n",
    "                                ))\n",
    "\n",
    "print(channel_data.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10155, 12)\n",
      "(10155, 12)\n",
      "(10155, 1)\n",
      "[[1.00000000e+000 1.33933645e-278 1.24641384e-068 ... 2.06991234e-225\n",
      "  6.92848298e-245 0.00000000e+000]\n",
      " [1.00000000e+000 0.00000000e+000 7.94968473e-289 ... 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]\n",
      " [1.00000000e+000 0.00000000e+000 1.26475152e-178 ... 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]\n",
      " ...\n",
      " [1.00000000e+000 0.00000000e+000 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]\n",
      " [1.00000000e+000 4.02727413e-315 4.37785181e-087 ... 1.14765653e-278\n",
      "  0.00000000e+000 0.00000000e+000]\n",
      " [1.00000000e+000 0.00000000e+000 9.10669404e-284 ... 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]]\n",
      "Epoch 0, Error: 23565.77791297299\n",
      "[[7.21052142e-228 0.00000000e+000 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]\n",
      " [0.00000000e+000 0.00000000e+000 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]\n",
      " [0.00000000e+000 0.00000000e+000 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]\n",
      " ...\n",
      " [0.00000000e+000 0.00000000e+000 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]\n",
      " [0.00000000e+000 0.00000000e+000 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]\n",
      " [0.00000000e+000 0.00000000e+000 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Test Accuracy: 0.002363367799113737\n"
     ]
    }
   ],
   "source": [
    "#Cortar en features y labales\n",
    "channel_data_train_samples = channel_data.shape[0]\n",
    "features = channel_data  # Features for training\n",
    "labels = train_data[:train_samples, -1]   # Labels for training\n",
    "\n",
    "features = cp.array(features)\n",
    "labels = cp.array(labels, ndmin=2)\n",
    "labels = labels.reshape(-1, 1)  # Reshape to (299, 1)\n",
    "\n",
    "print(channel_data.shape)\n",
    "print(features.shape )\n",
    "print(labels.shape)\n",
    "\n",
    "\n",
    "indices = cp.arange(channel_data_train_samples)\n",
    "X_train = cp.array(features)\n",
    "y_train = cp.array(labels)\n",
    "\n",
    "\n",
    "\n",
    "num_classes = 10\n",
    "input_size = 12\n",
    "hidden_layers = [256,256]  # Tamaños de las capas ocultas\n",
    "output_size = 10\n",
    "layer_sizes = [input_size] + hidden_layers + [output_size]\n",
    "epochs = 100\n",
    "learning_rate = 0.01\n",
    "dropout_rate = 0.2\n",
    "\n",
    "model = MultiLayerNetwork(layer_sizes)\n",
    "errors = model.train(X_train, y_train, epochs, learning_rate, 2.5)\n",
    "#Evaluar\n",
    "predictions = model.predict(X_train)\n",
    "\n",
    "accuracy = cp.mean(predictions.argmax(axis=1) == y_train)\n",
    "print(f'Test Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UTIL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data prep pero cortando 20% para test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtener datos (estoy usando pd porque anda considerablemente más rápido que np)\n",
    "train_data = pd.read_csv(r'C:/Users/kueru/Documents/VSCode/semestre_9/Deep_Learning/T2/train_data_2.csv')\n",
    "train_data = train_data.to_numpy()\n",
    "\n",
    "#Cortar en features y labales\n",
    "train_samples = train_data.shape[0]\n",
    "features = train_data[:train_samples, :-1]  # Features for training\n",
    "labels = train_data[:train_samples, -1]   # Labels for training\n",
    "\n",
    "features = cp.array(features)\n",
    "labels = cp.array(labels, ndmin=2)\n",
    "labels = labels.reshape(-1, 1)  # Reshape to (299, 1)\n",
    "\n",
    "print(train_data.shape)\n",
    "print(features.shape )\n",
    "print(labels.shape)\n",
    "\n",
    "\n",
    "indices = cp.arange(train_samples)\n",
    "\n",
    "#cortar 20% test \n",
    "test_size = 0.2\n",
    "test_samples = int(test_size * train_samples)\n",
    "train_indices, test_indices = indices[test_samples:], indices[:test_samples]\n",
    "\n",
    "X_train, X_test = features[train_indices], features[test_indices]\n",
    "y_train, y_test = labels[train_indices], labels[test_indices]\n",
    "\n",
    "train_indices = cp.array(train_indices)\n",
    "test_indices = cp.array(test_indices)\n",
    "X_train = cp.array(X_train)\n",
    "X_test = cp.array(X_test)\n",
    "y_train = cp.array(y_train)\n",
    "y_test = cp.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Obtener datos (estoy usando pd porque anda considerablemente más rápido que np)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mC:/Users/kueru/Documents/VSCode/semestre_9/Deep_Learning/T2/train_data.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m train_data \u001b[38;5;241m=\u001b[39m train_data\u001b[38;5;241m.\u001b[39mto_numpy()\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#Cortar en features y labales\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kueru\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kueru\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kueru\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\kueru\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:236\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    234\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mread_low_memory(nrows)\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m--> 236\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43m_concatenate_chunks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    239\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mread(nrows)\n",
      "File \u001b[1;32mc:\\Users\\kueru\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:376\u001b[0m, in \u001b[0;36m_concatenate_chunks\u001b[1;34m(chunks)\u001b[0m\n\u001b[0;32m    374\u001b[0m     result[name] \u001b[38;5;241m=\u001b[39m union_categoricals(arrs, sort_categories\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 376\u001b[0m     result[name] \u001b[38;5;241m=\u001b[39m \u001b[43mconcat_compat\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(non_cat_dtypes) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m result[name]\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;28mobject\u001b[39m):\n\u001b[0;32m    378\u001b[0m         warning_columns\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mstr\u001b[39m(name))\n",
      "File \u001b[1;32mc:\\Users\\kueru\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\dtypes\\concat.py:78\u001b[0m, in \u001b[0;36mconcat_compat\u001b[1;34m(to_concat, axis, ea_compat_axis)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m     77\u001b[0m     to_concat_arrs \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSequence[np.ndarray]\u001b[39m\u001b[38;5;124m\"\u001b[39m, to_concat)\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mto_concat_arrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m to_concat_eas \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSequence[ExtensionArray]\u001b[39m\u001b[38;5;124m\"\u001b[39m, to_concat)\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ea_compat_axis:\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;66;03m# We have 1D objects, that don't support axis keyword\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Obtener datos (estoy usando pd porque anda considerablemente más rápido que np)\n",
    "train_data = pd.read_csv(r'C:/Users/kueru/Documents/VSCode/semestre_9/Deep_Learning/T2/train_data.csv')\n",
    "train_data = train_data.to_numpy()\n",
    "    \n",
    "#Cortar en features y labales\n",
    "train_samples = train_data.shape[0]\n",
    "features = train_data[:train_samples, 1:-1]  # Features for training\n",
    "labels = train_data[:train_samples, -1]   # Labels for training\n",
    "\n",
    "# Reduce pixel values\n",
    "features = features / 255.0 \n",
    "# flatten the label values\n",
    "labels = labels.flatten()\n",
    "\n",
    "features = cp.array(features)\n",
    "labels = cp.array(labels, ndmin=2)\n",
    "labels = labels.reshape(-1, 1)  # Reshape to (299, 1)\n",
    "\n",
    "print(train_data.shape)\n",
    "print(features.shape )\n",
    "print(labels.shape)\n",
    "\n",
    "\n",
    "indices = cp.arange(train_samples)\n",
    "\n",
    "\n",
    "X_train = cp.array(features)\n",
    "y_train = cp.array(labels)\n",
    "\n",
    "num_classes = 10\n",
    "input_size = 3072\n",
    "hidden_layers = [100, 100]  # Tamaños de las capas ocultas\n",
    "output_size = 10\n",
    "layer_sizes = [input_size] + hidden_layers + [output_size]\n",
    "\n",
    "model = MultiLayerNetwork(layer_sizes)\n",
    "epochs = 1\n",
    "learning_rate = 10e-15\n",
    "\n",
    "errors = model.train(X_train, y_train, epochs, learning_rate)\n",
    "print(errors)\n",
    "#Evaluar\n",
    "predictions = model.predict(X_train)\n",
    "accuracy = cp.mean(predictions.argmax(axis=1) == y_train)\n",
    "print(f'Test Accuracy: {accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
