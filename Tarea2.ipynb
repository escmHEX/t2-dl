{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-05-25T02:32:01.437400Z","iopub.status.busy":"2024-05-25T02:32:01.437001Z","iopub.status.idle":"2024-05-25T02:32:01.473283Z","shell.execute_reply":"2024-05-25T02:32:01.471517Z","shell.execute_reply.started":"2024-05-25T02:32:01.437370Z"},"trusted":true},"outputs":[],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import requests\n","import cupy as cp\n","from PIL import Image\n","\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["def sigmoid(x):\n","    return 1 / (1 + cp.exp(-x))\n","\n","def softmax(x):\n","    exps = cp.exp(x - cp.max(x))\n","    return exps / cp.sum(exps, axis=1, keepdims=True)\n","\n","def sigmoid_derivative(x):\n","    return x * (1 - x)"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["def cross_entropy_loss(y_true, y_pred):\n","    n_samples = y_true.shape[0]\n","    logp = - cp.log(y_pred[range(n_samples), y_true.argmax(axis=1)])\n","    loss = cp.sum(logp) / n_samples\n","    return loss"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["def one_hot(y, num_classes):\n","    if y.ndim > 1:  # Flatten the array if necessary\n","        y = y.flatten()\n","        \n","    one_hot_labels = cp.zeros((y.shape[0], num_classes))\n","    one_hot_labels[cp.arange(y.shape[0]), y] = 1\n","    return one_hot_labels"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["class NeuralNetwork:\n","    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n","        self.input_size = input_size\n","        self.hidden_size1 = hidden_size1\n","        self.hidden_size2 = hidden_size2\n","        self.output_size = output_size\n","\n","        # Initialize weights and biases\n","        self.W1 = cp.random.randn(input_size, hidden_size1) * 0.01\n","        self.b1 = cp.zeros((1, hidden_size1))\n","        self.W2 = cp.random.randn(hidden_size1, hidden_size2) * 0.01\n","        self.b2 = cp.zeros((1, hidden_size2))\n","        self.W3 = cp.random.randn(hidden_size2, output_size) * 0.01\n","        self.b3 = cp.zeros((1, output_size))\n","\n","    def forward(self, X):\n","        self.z1 = cp.dot(X, self.W1) + self.b1\n","        self.a1 = sigmoid(self.z1)\n","        self.z2 = cp.dot(self.a1, self.W2) + self.b2\n","        self.a2 = sigmoid(self.z2)\n","        self.z3 = cp.dot(self.a2, self.W3) + self.b3\n","        self.a3 = softmax(self.z3)\n","        return self.a3\n","\n","    def backward(self, X, y_true, learning_rate):\n","        m = y_true.shape[0]\n","        y_pred = self.a3\n","\n","        # Calculate gradients\n","        dz3 = y_pred - y_true\n","        dW3 = cp.dot(self.a2.T, dz3) / m\n","        db3 = cp.sum(dz3, axis=0, keepdims=True) / m\n","\n","        dz2 = cp.dot(dz3, self.W3.T) * sigmoid_derivative(self.a2)\n","        dW2 = cp.dot(self.a1.T, dz2) / m\n","        db2 = cp.sum(dz2, axis=0, keepdims=True) / m\n","\n","        dz1 = cp.dot(dz2, self.W2.T) * sigmoid_derivative(self.a1)\n","        dW1 = cp.dot(X.T, dz1) / m\n","        db1 = cp.sum(dz1, axis=0, keepdims=True) / m\n","\n","        # Update weights and biases\n","        self.W3 -= learning_rate * dW3\n","        self.b3 -= learning_rate * db3\n","        self.W2 -= learning_rate * dW2\n","        self.b2 -= learning_rate * db2\n","        self.W1 -= learning_rate * dW1\n","        self.b1 -= learning_rate * db1\n","\n","    def train(self, X, y, epochs, learning_rate):\n","        y_one_hot = one_hot(y, self.output_size)\n","\n","        for epoch in range(epochs):\n","            y_pred = self.forward(X)\n","            loss = cross_entropy_loss(y_one_hot, y_pred)\n","            self.backward(X, y_one_hot, learning_rate)\n","\n","            if epoch % 100 == 0:\n","                print(f'Epoch {epoch}, Loss: {loss}')\n","\n","    def predict(self, X):\n","        y_pred = self.forward(X)\n","        return np.argmax(y_pred, axis=1)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-05-25T02:31:10.867590Z","iopub.status.busy":"2024-05-25T02:31:10.866795Z","iopub.status.idle":"2024-05-25T02:31:36.681189Z","shell.execute_reply":"2024-05-25T02:31:36.679408Z","shell.execute_reply.started":"2024-05-25T02:31:10.867551Z"},"trusted":true},"outputs":[],"source":["train_data = pd.read_csv('train_data.csv')\n","train_data = train_data.to_numpy()\n","    \n","#Cortar en features y labales\n","train_samples = train_data.shape[0]\n","features = train_data[:train_samples, 1:-1]  # Features for training    \n","labels = train_data[:train_samples, -1]   # Labels for training\n","\n","features = cp.array(features)\n","labels = cp.array(labels, ndmin=2)\n","labels = labels.reshape(-1, 1)  # Reshape to (299, 1)\n","\n","indices = cp.arange(train_samples)\n","\n","X_train = cp.array(features)\n","y_train = cp.array(labels).flatten()"]},{"cell_type":"markdown","metadata":{},"source":["red_pixels = []\n","green_pixels = []\n","blue_pixels = []\n","train_data = train_data[:, 1:-1]\n","\n","# Iterate through each row in the original matrix\n","for row in train_data:\n","    pr = []\n","    pg = []\n","    pb = []\n","    for i in range(len(row)):\n","        if i % 3 == 0:\n","            pr.append(row[i])\n","        \n","        if i % 3 == 1:\n","            pg.append(row[i])\n","\n","        if i % 3 == 2:\n","            pb.append(row[i])\n","\n","    red_pixels.append(pr)\n","    green_pixels.append(pg)\n","    blue_pixels.append(pb)"]},{"cell_type":"markdown","metadata":{},"source":["#intensidad media por conal\n","mean_red = np.mean(red_pixels, axis=1)\n","mean_green = np.mean(green_pixels, axis=1)\n","mean_blue = np.mean(blue_pixels, axis=1)\n","\n","#varianza\n","var_red = np.var(red_pixels, axis=1)\n","var_green = np.var(green_pixels, axis=1)\n","var_blue = np.var(blue_pixels, axis=1)\n","\n","#desviación estándar\n","std_red = np.std(red_pixels, axis=1)\n","std_green = np.std(green_pixels, axis=1)\n","std_blue = np.std(blue_pixels, axis=1)\n","\n","#contraste\n","contrast_red = np.ptp(red_pixels, axis=1) # ptp: peak to peak (max - min)\n","contrast_green = np.ptp(green_pixels, axis=1)\n","contrast_blue = np.ptp(blue_pixels, axis=1)\n","\n","channel_data = np.column_stack((\n","                                mean_red, mean_green, mean_blue,\n","                                var_red, var_green, var_blue,\n","                                std_red, std_green, std_blue,\n","                                contrast_red, contrast_green, contrast_blue\n","                                ))\n","\n","print(channel_data.shape)"]},{"cell_type":"markdown","metadata":{},"source":["#Cortar en features y labales\n","channel_data_train_samples = channel_data.shape[0]\n","features = channel_data  # Features for training\n","\n","features = cp.array(features)\n","\n","print(channel_data.shape)\n","print(features.shape)\n","print(labels.shape)\n","\n","X_train = cp.array(features)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-05-25T02:29:51.001856Z","iopub.status.busy":"2024-05-25T02:29:51.001413Z","iopub.status.idle":"2024-05-25T02:30:17.277529Z","shell.execute_reply":"2024-05-25T02:30:17.276240Z","shell.execute_reply.started":"2024-05-25T02:29:51.001823Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0, Loss: 2.3045199373070764\n","Epoch 100, Loss: 2.289785783213048\n","Epoch 200, Loss: 2.1926823025760247\n","[6 6 8 ... 6 6 0]\n","Test Accuracy: 0.19154901960784312\n"]}],"source":["# Hyperparameters\n","input_size = 3072\n","hidden_size1 = 256\n","hidden_size2 = 256\n","output_size = 10\n","epochs = 1000\n","learning_rate = 0.01\n","\n","# Initialize and train the neural network\n","nn = NeuralNetwork(input_size, hidden_size1, hidden_size2, output_size)\n","nn.train(X_train, y_train, epochs, learning_rate)\n","\n","# Make predictions\n","predictions = nn.predict(X_train)\n","print(predictions)\n","\n","accuracy = cp.mean(predictions == y_train)\n","print(f'Test Accuracy: {accuracy}')"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":8425536,"sourceId":77334,"sourceType":"competition"}],"dockerImageVersionId":30698,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3.11.3 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"},"vscode":{"interpreter":{"hash":"c261aea317cc0286b3b3261fbba9abdec21eaa57589985bb7a274bf54d6cc0a7"}}},"nbformat":4,"nbformat_minor":4}
