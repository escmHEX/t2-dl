{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función de activación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + cp.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def softmax(z):#z: vector resultante antes de funcion de activación en la última capa\n",
    "    exp_z = cp.exp(z - cp.max(z, axis=1, keepdims=True))  \n",
    "    return exp_z / cp.sum(exp_z, axis=1, keepdims=True)#retorna las probabilidades de las posibles clases\n",
    "\n",
    "#mide la diferencia entre distribucion de prob. creada por softmax y los targets reales\n",
    "def cross_entropy_loss(predictions, targets):\n",
    "    return -cp.sum(targets * cp.log(predictions + 1e-9)) / targets.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder que cambia el formato de labels tal que coincida con las predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(y, num_classes):\n",
    "    if y.ndim > 1:  # Flatten the array if necessary\n",
    "        y = y.flatten()\n",
    "        \n",
    "    one_hot_labels = cp.zeros((y.shape[0], num_classes))\n",
    "    one_hot_labels[cp.arange(y.shape[0]), y] = 1\n",
    "    return one_hot_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clase MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiLayerNetwork:\n",
    "    def __init__(self, layer_sizes):\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "\n",
    "        # Initialize weights and biases for each layer\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            weight = cp.random.randn(layer_sizes[i], layer_sizes[i + 1]) * cp.sqrt(2. / layer_sizes[i])\n",
    "            bias = cp.zeros((1, layer_sizes[i + 1]))\n",
    "            self.weights.append(weight)\n",
    "            self.biases.append(bias)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.activations = [inputs]\n",
    "        a = inputs\n",
    "\n",
    "        for i in range(len(self.weights) - 1):\n",
    "            z = cp.dot(a, self.weights[i]) + self.biases[i]\n",
    "            a = sigmoid(z)\n",
    "            self.activations.append(a)\n",
    "\n",
    "        # Output uses softmax for multiclass classification\n",
    "        z = cp.dot(a, self.weights[-1]) + self.biases[-1]\n",
    "        a = softmax(z)\n",
    "        self.activations.append(a)\n",
    "        return a\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        inputs = cp.array(inputs, ndmin=2)\n",
    "        a = inputs\n",
    "\n",
    "        for i in range(len(self.weights) - 1):\n",
    "            z = cp.dot(a, self.weights[i]) + self.biases[i]\n",
    "            a = sigmoid(z)\n",
    "        \n",
    "        # Output with softmax\n",
    "        z = cp.dot(a, self.weights[-1]) + self.biases[-1]\n",
    "        a = softmax(z)\n",
    "\n",
    "        return a\n",
    "\n",
    "    def backward(self, targets, learning_rate):\n",
    "        m = targets.shape[0]  # number of training examples\n",
    "        delta_weights = [0] * len(self.weights)\n",
    "        delta_biases = [0] * len(self.biases)\n",
    "\n",
    "        # Calculate the initial error (difference between prediction and target for the output layer)\n",
    "        error = self.activations[-1] - targets\n",
    "\n",
    "        for i in reversed(range(len(self.weights))):\n",
    "            # Calculate the delta for the current layer\n",
    "            delta = error\n",
    "            delta_weights[i] = cp.dot(self.activations[i].T, delta) / m\n",
    "            delta_biases[i] = cp.sum(delta, axis=0, keepdims=True) / m\n",
    "\n",
    "            if i != 0:\n",
    "                # Propagate the error to the previous layer\n",
    "                error = cp.dot(delta, self.weights[i].T) * sigmoid_derivative(self.activations[i])\n",
    "\n",
    "            # Update weights and biases\n",
    "            self.weights[i] -= learning_rate * delta_weights[i]\n",
    "            self.biases[i] -= learning_rate * delta_biases[i]\n",
    "\n",
    "            \n",
    "    def train(self, inputs, targets, epochs, learning_rate):\n",
    "        targets = one_hot(targets, 10)\n",
    "\n",
    "        errors = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            predictions = self.forward(inputs)\n",
    "            error = cross_entropy_loss(predictions, targets)\n",
    "            self.backward(targets, learning_rate)\n",
    "            errors.append(error)\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                print(f'Epoch {epoch}, Error: {error}')\n",
    "\n",
    "        return errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51000, 3074)\n",
      "(51000, 3072)\n",
      "(51000, 1)\n"
     ]
    }
   ],
   "source": [
    "#Obtener datos (estoy usando pd porque anda considerablemente más rápido que np)\n",
    "train_data = pd.read_csv(r'train_data.csv')\n",
    "train_data = train_data.to_numpy()\n",
    "    \n",
    "#Cortar en features y labales\n",
    "train_samples = train_data.shape[0]\n",
    "features = train_data[:train_samples, 1:-1]  # Features for training    \n",
    "labels = train_data[:train_samples, -1]  #Labels for training\n",
    "\n",
    "labels = labels.reshape(-1, 1)  # Reshape to (299, 1)\n",
    "\n",
    "X_train = cp.array(features)\n",
    "y_train = cp.array(labels).flatten()\n",
    "\n",
    "print(train_data.shape)\n",
    "print(features.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Error: 2.434616586756469\n",
      "Epoch 10, Error: 2.2016131371311425\n",
      "Epoch 20, Error: 2.147303683400798\n",
      "Epoch 30, Error: 2.1059907710491026\n",
      "Epoch 40, Error: 2.0716307448243194\n",
      "Epoch 50, Error: 2.042959699512731\n",
      "Epoch 60, Error: 2.0180059806836033\n",
      "Epoch 70, Error: 1.9996770994787252\n",
      "Epoch 80, Error: 1.9876027903696372\n",
      "Epoch 90, Error: 1.9668331717220198\n",
      "Epoch 100, Error: 1.9494130841504729\n",
      "Epoch 110, Error: 1.9598391616711845\n",
      "Epoch 120, Error: 1.924952577757793\n",
      "Epoch 130, Error: 1.9148793024377004\n",
      "Epoch 140, Error: 1.9235766829625383\n",
      "Epoch 150, Error: 1.9021999998211392\n",
      "Epoch 160, Error: 1.8969955182916938\n",
      "Epoch 170, Error: 1.8845436886405007\n",
      "Epoch 180, Error: 1.8870631390831094\n",
      "Epoch 190, Error: 1.8956960514070356\n",
      "Epoch 200, Error: 1.8682549251757685\n",
      "Epoch 210, Error: 1.8677767123827986\n",
      "Epoch 220, Error: 1.839600233228658\n",
      "Epoch 230, Error: 1.8578504513418803\n",
      "Epoch 240, Error: 1.841699166172649\n",
      "Epoch 250, Error: 1.8250604872433152\n",
      "Epoch 260, Error: 1.8407677740399162\n",
      "Epoch 270, Error: 1.8209386199029105\n",
      "Epoch 280, Error: 1.8122991956854302\n",
      "Epoch 290, Error: 1.8232149327695806\n",
      "Test Accuracy: 0.3571960784313726\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "input_size = 3072\n",
    "hidden_layers = [1024, 512]  # Tamaños de las capas ocultas\n",
    "output_size = 10\n",
    "layer_sizes = [input_size] + hidden_layers + [output_size]\n",
    "\n",
    "model = MultiLayerNetwork(layer_sizes)\n",
    "epochs = 300\n",
    "learning_rate = 0.05\n",
    "\n",
    "errors = model.train(X_train, y_train, epochs, learning_rate)\n",
    "#Evaluar\n",
    "predictions = model.predict(X_train)\n",
    "accuracy = cp.mean(predictions.argmax(axis=1) == y_train)\n",
    "print(f'Test Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Búsqueda de parámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Searh para ajustar complejitud de 1 capa oculta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer size: 1\n",
      "[3072, 1, 10]\n",
      "Epoch 0, Error: 2.302621414426933\n",
      "Epoch 10, Error: 2.3025529936149853\n",
      "Epoch 20, Error: 2.302498165861214\n",
      "Epoch 30, Error: 2.302448581355747\n",
      "Epoch 40, Error: 2.3024037301658726\n",
      "Epoch 50, Error: 2.3023631576199195\n",
      "Epoch 60, Error: 2.302326454222536\n",
      "Epoch 70, Error: 2.302293250202761\n",
      "Epoch 80, Error: 2.3022632113558354\n",
      "Epoch 90, Error: 2.302236035506209\n",
      "Epoch 100, Error: 2.302211449387694\n",
      "Epoch 110, Error: 2.302189205853871\n",
      "Epoch 120, Error: 2.30216908136942\n",
      "Epoch 130, Error: 2.3021508737480465\n",
      "Epoch 140, Error: 2.3021344001100186\n",
      "Epoch 150, Error: 2.302119495036605\n",
      "Epoch 160, Error: 2.3021060089017698\n",
      "Epoch 170, Error: 2.3020938063636947\n",
      "Epoch 180, Error: 2.302082765000622\n",
      "Epoch 190, Error: 2.3020727740770814\n",
      "Test Accuracy: 0.1069423929098966\n",
      "\n",
      "layer size: 10\n",
      "[3072, 10, 10]\n",
      "Epoch 0, Error: 2.579213115582387\n",
      "Epoch 10, Error: 2.3516751832167127\n",
      "Epoch 20, Error: 2.333705394466311\n",
      "Epoch 30, Error: 2.322258709739571\n",
      "Epoch 40, Error: 2.314787074289977\n",
      "Epoch 50, Error: 2.3099065153749825\n",
      "Epoch 60, Error: 2.3068093620290826\n",
      "Epoch 70, Error: 2.3048376121890515\n",
      "Epoch 80, Error: 2.30355701515815\n",
      "Epoch 90, Error: 2.3021764050934914\n",
      "Epoch 100, Error: 2.3025496567828765\n",
      "Epoch 110, Error: 2.3022545787922755\n",
      "Epoch 120, Error: 2.302072624960121\n",
      "Epoch 130, Error: 2.3019612215688063\n",
      "Epoch 140, Error: 2.301893341745262\n",
      "Epoch 150, Error: 2.3018519938550175\n",
      "Epoch 160, Error: 2.301826835675281\n",
      "Epoch 170, Error: 2.3018115331431632\n",
      "Epoch 180, Error: 2.3018022173661676\n",
      "Epoch 190, Error: 2.301796532110569\n",
      "Test Accuracy: 0.10674544559330379\n",
      "\n",
      "layer size: 100\n",
      "[3072, 100, 10]\n",
      "Epoch 0, Error: 2.4679771701467055\n",
      "Epoch 10, Error: 2.239840387715534\n",
      "Epoch 20, Error: 2.1792798106556077\n",
      "Epoch 30, Error: 2.1735837168593535\n",
      "Epoch 40, Error: 2.13901838029953\n",
      "Epoch 50, Error: 2.1058693634972276\n",
      "Epoch 60, Error: 2.11115910025999\n",
      "Epoch 70, Error: 2.075914006976682\n",
      "Epoch 80, Error: 2.0777553072163424\n",
      "Epoch 90, Error: 2.054989200590509\n",
      "Epoch 100, Error: 2.0453986723303115\n",
      "Epoch 110, Error: 2.0223191607306763\n",
      "Epoch 120, Error: 2.0360489231379284\n",
      "Epoch 130, Error: 2.062887127210393\n",
      "Epoch 140, Error: 1.974741568616573\n",
      "Epoch 150, Error: 1.9866385928220927\n",
      "Epoch 160, Error: 2.0199023926393997\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(layer_sizes)\n\u001b[0;32m     11\u001b[0m model \u001b[38;5;241m=\u001b[39m MultiLayerNetwork(layer_sizes)\n\u001b[1;32m---> 12\u001b[0m errors \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m#Evaluar\u001b[39;00m\n\u001b[0;32m     15\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_train)\n",
      "Cell \u001b[1;32mIn[11], line 78\u001b[0m, in \u001b[0;36mMultiLayerNetwork.train\u001b[1;34m(self, inputs, targets, epochs, learning_rate)\u001b[0m\n\u001b[0;32m     75\u001b[0m     errors\u001b[38;5;241m.\u001b[39mappend(error)\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 78\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Error: \u001b[39m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43merror\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m errors\n",
      "File \u001b[1;32mcupy\\_core\\core.pyx:1744\u001b[0m, in \u001b[0;36mcupy._core.core._ndarray_base.__format__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "epochs = 200\n",
    "layer_sizes = [input_size] + hidden_layers + [output_size]\n",
    "lr = 0.05\n",
    "for i in range(0, 5):  # Cambia el 5 por el número de potencias de 10 que desees\n",
    "    val = pow(10, i)\n",
    "    hidden_layers = [val]  # Tamaños de las capas ocultas\n",
    "    print(f\"layer size: {val}\")\n",
    "    layer_sizes = [input_size] + hidden_layers + [output_size]\n",
    "    print(layer_sizes)\n",
    "\n",
    "    model = MultiLayerNetwork(layer_sizes)\n",
    "    errors = model.train(X_train, y_train, epochs, lr)\n",
    "\n",
    "    #Evaluar\n",
    "    predictions = model.predict(X_train)\n",
    "    accuracy = cp.mean(predictions.argmax(axis=1) == y_train)\n",
    "    print(f'Test Accuracy: {accuracy}')\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search para ajustar numero de capas ocultas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden layers: [256]\n",
      "Epoch 0, Error: 105.86025886841283\n",
      "Test Accuracy: 0.09698996655518395\n",
      "\n",
      "hidden layers: [256, 256]\n",
      "Epoch 0, Error: 108.85456644116445\n",
      "Test Accuracy: 0.14046822742474915\n",
      "\n",
      "hidden layers: [256, 256, 256]\n",
      "Epoch 0, Error: 105.7057006341719\n",
      "Test Accuracy: 0.14046822742474915\n",
      "\n",
      "hidden layers: [256, 256, 256, 256]\n",
      "Epoch 0, Error: 110.30727016766471\n",
      "Test Accuracy: 0.14046822742474915\n",
      "\n",
      "hidden layers: [256, 256, 256, 256, 256]\n",
      "Epoch 0, Error: 125.7849884262953\n",
      "Test Accuracy: 0.14046822742474915\n",
      "\n",
      "hidden layers: [256, 256, 256, 256, 256, 256]\n",
      "Epoch 0, Error: 111.10845764177878\n",
      "Test Accuracy: 0.14046822742474915\n",
      "\n",
      "hidden layers: [256, 256, 256, 256, 256, 256, 256]\n",
      "Epoch 0, Error: 106.9112295635407\n",
      "Test Accuracy: 0.14046822742474915\n",
      "\n",
      "hidden layers: [256, 256, 256, 256, 256, 256, 256, 256]\n",
      "Epoch 0, Error: 107.25215446902249\n",
      "Test Accuracy: 0.14046822742474915\n",
      "\n",
      "hidden layers: [256, 256, 256, 256, 256, 256, 256, 256, 256]\n",
      "Epoch 0, Error: 105.26643352242563\n",
      "Test Accuracy: 0.14046822742474915\n",
      "\n",
      "hidden layers: [256, 256, 256, 256, 256, 256, 256, 256, 256, 256]\n",
      "Epoch 0, Error: 120.37974780141502\n",
      "Test Accuracy: 0.14046822742474915\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_classes = 10\n",
    "input_size = 3072\n",
    "hidden_layer_size = 256\n",
    "hidden_layers = []  # Tamaños de las capas ocultas\n",
    "output_size = 10\n",
    "layer_sizes = [input_size] + hidden_layers + [output_size]\n",
    "epochs = 100\n",
    "lr = 0.0001\n",
    "\n",
    "for i in range(0, 10):  # Cambia el 5 por el número de potencias de 10 que desees\n",
    "    hidden_layers.append(hidden_layer_size)  # Tamaños de las capas ocultas\n",
    "    print(f\"hidden layers: {hidden_layers}\")\n",
    "    layer_sizes = [input_size] + hidden_layers + [output_size]\n",
    "\n",
    "    model = MultiLayerNetwork(layer_sizes)\n",
    "    errors = model.train(X_train, y_train, epochs, lr)\n",
    "\n",
    "    #Evaluar\n",
    "    predictions = model.predict(X_train)\n",
    "    accuracy = cp.mean(predictions.argmax(axis=1) == y_train)\n",
    "    print(f'Test Accuracy: {accuracy}')\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo entrenado con parámetros encontrados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularizaciones:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiLayerNetwork_Dropout:\n",
    "    def __init__(self, layer_sizes, dropout_rate=0.5):\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.dropout_masks = []\n",
    "\n",
    "        # Initialize weights and biases for each layer\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            weight = cp.random.randn(layer_sizes[i], layer_sizes[i + 1]) * cp.sqrt(2. / layer_sizes[i])\n",
    "            bias = cp.zeros((1, layer_sizes[i + 1]))\n",
    "            self.weights.append(weight)\n",
    "            self.biases.append(bias)\n",
    "            # Initialize dropout mask\n",
    "            self.dropout_masks.append(None)\n",
    "\n",
    "    def forward(self, inputs, training=True):\n",
    "        self.activations = [inputs]\n",
    "        a = inputs\n",
    "\n",
    "        for i in range(len(self.weights) - 1):\n",
    "            z = cp.dot(a, self.weights[i]) + self.biases[i]\n",
    "            a = sigmoid(z)\n",
    "\n",
    "            if training:\n",
    "                # Apply dropout\n",
    "                dropout_mask = (cp.random.rand(*a.shape) > self.dropout_rate) / (1.0 - self.dropout_rate)\n",
    "                a *= dropout_mask\n",
    "                self.dropout_masks[i] = dropout_mask\n",
    "\n",
    "            self.activations.append(a)\n",
    "\n",
    "        # Output uses softmax for multiclass classification\n",
    "        z = cp.dot(a, self.weights[-1]) + self.biases[-1]\n",
    "        a = softmax(z)\n",
    "        self.activations.append(a)\n",
    "        return a\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        inputs = cp.array(inputs, ndmin=2)\n",
    "        return self.forward(inputs, training=False)\n",
    "\n",
    "    def backward(self, targets, learning_rate):\n",
    "        m = targets.shape[0]  # number of training examples\n",
    "        delta_weights = [0] * len(self.weights)\n",
    "        delta_biases = [0] * len(self.biases)\n",
    "\n",
    "        # Calculate the initial error (difference between prediction and target for the output layer)\n",
    "        error = self.activations[-1] - targets\n",
    "\n",
    "        for i in reversed(range(len(self.weights))):\n",
    "            # Calculate the delta for the current layer\n",
    "            delta = error\n",
    "            delta_weights[i] = cp.dot(self.activations[i].T, delta) / m\n",
    "            delta_biases[i] = cp.sum(delta, axis=0, keepdims=True) / m\n",
    "\n",
    "            if i != 0:\n",
    "                # Propagate the error to the previous layer\n",
    "                error = cp.dot(delta, self.weights[i].T) * sigmoid_derivative(self.activations[i])\n",
    "                if self.dropout_masks[i-1] is not None:\n",
    "                    error *= self.dropout_masks[i-1]\n",
    "\n",
    "            # Update weights and biases\n",
    "            self.weights[i] -= learning_rate * delta_weights[i]\n",
    "            self.biases[i] -= learning_rate * delta_biases[i]\n",
    "\n",
    "    def train(self, inputs, targets, epochs, learning_rate):\n",
    "        targets = one_hot(targets, 10)\n",
    "\n",
    "        errors = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            predictions = self.forward(inputs, training=True)\n",
    "            error = cross_entropy_loss(predictions, targets)\n",
    "            self.backward(targets, learning_rate)\n",
    "            errors.append(error)\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                print(f'Epoch {epoch}, Error: {error}')\n",
    "\n",
    "        return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Error: 2.6304590288090957\n",
      "Epoch 10, Error: 2.416289035108642\n",
      "Epoch 20, Error: 2.435132986179687\n",
      "Epoch 30, Error: 2.4479345915304282\n",
      "Epoch 40, Error: 2.451278092300832\n",
      "Epoch 50, Error: 2.44242108255252\n",
      "Epoch 60, Error: 2.432301446343906\n",
      "Epoch 70, Error: 2.4304058688526666\n",
      "Epoch 80, Error: 2.422964107740824\n",
      "Epoch 90, Error: 2.4141150109396317\n",
      "Epoch 100, Error: 2.4118779038003746\n",
      "Epoch 110, Error: 2.401839477399119\n",
      "Epoch 120, Error: 2.3984448654187545\n",
      "Epoch 130, Error: 2.394719815923066\n",
      "Epoch 140, Error: 2.387000660320394\n",
      "Epoch 150, Error: 2.3858565064480373\n",
      "Epoch 160, Error: 2.3812473908587264\n",
      "Epoch 170, Error: 2.3785743566890107\n",
      "Epoch 180, Error: 2.378455048475347\n",
      "Epoch 190, Error: 2.378545869353678\n",
      "Epoch 200, Error: 2.374787308657348\n",
      "Epoch 210, Error: 2.3716144230227494\n",
      "Epoch 220, Error: 2.3708065203260587\n",
      "Epoch 230, Error: 2.3644337455754894\n",
      "Epoch 240, Error: 2.3631216344800503\n",
      "Epoch 250, Error: 2.36319663858445\n",
      "Epoch 260, Error: 2.3599197629709803\n",
      "Epoch 270, Error: 2.3570970938178006\n",
      "Epoch 280, Error: 2.357571254315256\n",
      "Epoch 290, Error: 2.3549378018638705\n",
      "Test Accuracy: 0.10747058823529412\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "input_size = 3072\n",
    "hidden_layers = [1024, 512]  # Tamaños de las capas ocultas\n",
    "output_size = 10\n",
    "layer_sizes = [input_size] + hidden_layers + [output_size]\n",
    "epochs = 300\n",
    "learning_rate = 0.05\n",
    "dropout_rate = 0.2\n",
    "\n",
    "\n",
    "model = MultiLayerNetwork_Dropout(layer_sizes, dropout_rate)\n",
    "errors = model.train(X_train, y_train, epochs, learning_rate)\n",
    "#Evaluar\n",
    "predictions = model.predict(X_train)\n",
    "accuracy = cp.mean(predictions.argmax(axis=1) == y_train)\n",
    "print(f'Test Accuracy: {accuracy}')\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Porcentaje para cada canal RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_pixels = []\n",
    "green_pixels = []\n",
    "blue_pixels = []\n",
    "train_data = train_data[:, 1:-1]\n",
    "\n",
    "# Iterate through each row in the original matrix\n",
    "for row in train_data:\n",
    "    pr = []\n",
    "    pg = []\n",
    "    pb = []\n",
    "    for i in range(len(row)):\n",
    "        if i % 3 == 0:\n",
    "            pr.append(row[i])\n",
    "        \n",
    "        if i % 3 == 1:\n",
    "            pg.append(row[i])\n",
    "\n",
    "        if i % 3 == 2:\n",
    "            pb.append(row[i])\n",
    "\n",
    "    red_pixels.append(pr)\n",
    "    green_pixels.append(pg)\n",
    "    blue_pixels.append(pb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Más datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51000, 12)\n"
     ]
    }
   ],
   "source": [
    "#intensidad media por conal\n",
    "mean_red = np.mean(red_pixels, axis=1)\n",
    "mean_green = np.mean(green_pixels, axis=1)\n",
    "mean_blue = np.mean(blue_pixels, axis=1)\n",
    "\n",
    "#varianza\n",
    "var_red = np.var(red_pixels, axis=1)\n",
    "var_green = np.var(green_pixels, axis=1)\n",
    "var_blue = np.var(blue_pixels, axis=1)\n",
    "\n",
    "#desviación estándar\n",
    "std_red = np.std(red_pixels, axis=1)\n",
    "std_green = np.std(green_pixels, axis=1)\n",
    "std_blue = np.std(blue_pixels, axis=1)\n",
    "\n",
    "#contraste\n",
    "contrast_red = np.ptp(red_pixels, axis=1) # ptp: peak to peak (max - min)\n",
    "contrast_green = np.ptp(green_pixels, axis=1)\n",
    "contrast_blue = np.ptp(blue_pixels, axis=1)\n",
    "\n",
    "channel_data = np.column_stack((\n",
    "                                mean_red, mean_green, mean_blue,\n",
    "                                var_red, var_green, var_blue,\n",
    "                                std_red, std_green, std_blue,\n",
    "                                contrast_red, contrast_green, contrast_blue\n",
    "                                ))\n",
    "\n",
    "print(channel_data.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cortar en features y labales\n",
    "channel_data_train_samples = channel_data.shape[0]\n",
    "features = channel_data  # Features for training\n",
    "\n",
    "features = cp.array(features)\n",
    "\n",
    "print(channel_data.shape)\n",
    "print(features.shape )\n",
    "print(labels.shape)\n",
    "\n",
    "X_train = cp.append(X_train, cp.array(features), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train modelo con dataset de parámetros por imagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Error: 2.5113275348122652\n",
      "Epoch 10, Error: 2.2445904367446388\n",
      "Epoch 20, Error: 2.238371108577897\n",
      "Epoch 30, Error: 2.186311058398496\n",
      "Epoch 40, Error: 2.1144387106637783\n",
      "Epoch 50, Error: 2.079136299688558\n",
      "Epoch 60, Error: 2.046947599714221\n",
      "Epoch 70, Error: 2.0257240532966465\n",
      "Epoch 80, Error: 2.018456819466521\n",
      "Epoch 90, Error: 1.9744938873579285\n",
      "Epoch 100, Error: 1.9919819534151637\n",
      "Epoch 110, Error: 1.990961507554027\n",
      "Epoch 120, Error: 1.9689962318405776\n",
      "Epoch 130, Error: 1.973475511774524\n",
      "Epoch 140, Error: 1.9230950615772477\n",
      "Epoch 150, Error: 1.9318461595505794\n",
      "Epoch 160, Error: 1.8970906832134782\n",
      "Epoch 170, Error: 1.9211776316373792\n",
      "Epoch 180, Error: 1.861067385326333\n",
      "Epoch 190, Error: 1.8655231958553895\n",
      "Epoch 200, Error: 1.9612651063469637\n",
      "Epoch 210, Error: 1.8733193397794161\n",
      "Epoch 220, Error: 1.881340530620989\n",
      "Epoch 230, Error: 1.8517431658204806\n",
      "Epoch 240, Error: 1.880459759175634\n",
      "Epoch 250, Error: 1.8315925189345055\n",
      "Epoch 260, Error: 1.8183835988981012\n",
      "Epoch 270, Error: 1.8251942115872744\n",
      "Epoch 280, Error: 1.8515468329533007\n",
      "Epoch 290, Error: 1.862746656384268\n",
      "Epoch 300, Error: 1.8127790389261949\n",
      "Epoch 310, Error: 1.7835513111662418\n",
      "Epoch 320, Error: 1.7940958839007506\n",
      "Epoch 330, Error: 1.8234581424807856\n",
      "Epoch 340, Error: 1.8013980750830096\n",
      "Epoch 350, Error: 1.7625903112801664\n",
      "Epoch 360, Error: 1.7829560505670208\n",
      "Epoch 370, Error: 1.8601479514640358\n",
      "Epoch 380, Error: 1.7677175927750197\n",
      "Epoch 390, Error: 1.7477216128134725\n",
      "Epoch 400, Error: 1.8047068283206762\n",
      "Epoch 410, Error: 1.7436027031152388\n",
      "Epoch 420, Error: 1.7692151796797824\n",
      "Epoch 430, Error: 1.7338433253610515\n",
      "Epoch 440, Error: 1.7434679962939623\n",
      "Epoch 450, Error: 1.7521186447908943\n",
      "Epoch 460, Error: 1.7183634872938456\n",
      "Epoch 470, Error: 1.7667946003171884\n",
      "Epoch 480, Error: 1.7243008711849823\n",
      "Epoch 490, Error: 1.7245599598537604\n",
      "Epoch 500, Error: 1.7453846156244983\n",
      "Epoch 510, Error: 1.7339146894944482\n",
      "Epoch 520, Error: 1.7178641386721794\n",
      "Epoch 530, Error: 1.7077146881751182\n",
      "Epoch 540, Error: 1.6916700956169644\n",
      "Epoch 550, Error: 1.720022532520416\n",
      "Epoch 560, Error: 1.6897023502747834\n",
      "Epoch 570, Error: 1.6915358763827755\n",
      "Epoch 580, Error: 1.6845396870881209\n",
      "Epoch 590, Error: 1.6941055898533965\n",
      "Epoch 600, Error: 1.674082848819122\n",
      "Epoch 610, Error: 1.687347548001172\n",
      "Epoch 620, Error: 1.6825122310007827\n",
      "Epoch 630, Error: 1.6789593263065183\n",
      "Epoch 640, Error: 1.7100848823563646\n",
      "Epoch 650, Error: 1.69979151852084\n",
      "Epoch 660, Error: 1.643475427789338\n",
      "Epoch 670, Error: 1.7214873956435972\n",
      "Epoch 680, Error: 1.7061750796090862\n",
      "Epoch 690, Error: 1.6546484594898199\n",
      "Epoch 700, Error: 1.6632452029056979\n",
      "Epoch 710, Error: 1.6675989666902786\n",
      "Epoch 720, Error: 1.6842069611712023\n",
      "Epoch 730, Error: 1.6746257963862907\n",
      "Epoch 740, Error: 1.6838389970807492\n",
      "Epoch 750, Error: 1.713559146420451\n",
      "Epoch 760, Error: 1.6496894794752175\n",
      "Epoch 770, Error: 1.639944479740868\n",
      "Epoch 780, Error: 1.6564042700876098\n",
      "Epoch 790, Error: 1.6707671960865766\n",
      "Epoch 800, Error: 1.6481444977141102\n",
      "Epoch 810, Error: 1.6561338701262445\n",
      "Epoch 820, Error: 1.6714479340296708\n",
      "Epoch 830, Error: 1.6616017881282086\n",
      "Epoch 840, Error: 1.6361001386116043\n",
      "Epoch 850, Error: 1.6308114323449194\n",
      "Epoch 860, Error: 1.6845026901366145\n",
      "Epoch 870, Error: 1.6519859152938006\n",
      "Epoch 880, Error: 1.6282300572707755\n",
      "Epoch 890, Error: 1.6325398460615326\n",
      "Epoch 900, Error: 1.6119005884044173\n",
      "Epoch 910, Error: 1.6725005405484852\n",
      "Epoch 920, Error: 1.6191015286372297\n",
      "Epoch 930, Error: 1.633379490807909\n",
      "Epoch 940, Error: 1.654075220813133\n",
      "Epoch 950, Error: 1.6259613622645004\n",
      "Epoch 960, Error: 1.5873033525382854\n",
      "Epoch 970, Error: 1.5992721992475305\n",
      "Epoch 980, Error: 1.598365685129146\n",
      "Epoch 990, Error: 1.6511317456960461\n",
      "Test Accuracy: 0.4341764705882353\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "input_size = 3084\n",
    "hidden_layers = [1024, 512]  # Tamaños de las capas ocultas\n",
    "output_size = 10\n",
    "layer_sizes = [input_size] + hidden_layers + [output_size]\n",
    "epochs = 1000\n",
    "learning_rate = 0.1\n",
    "dropout_rate = 0.2\n",
    "\n",
    "model = MultiLayerNetwork(layer_sizes)\n",
    "errors = model.train(X_train, y_train, epochs, learning_rate)\n",
    "#Evaluar\n",
    "predictions = model.predict(X_train)\n",
    "\n",
    "accuracy = cp.mean(predictions.argmax(axis=1) == y_train)\n",
    "print(f'Test Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UTIL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data prep pero cortando 20% para test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtener datos (estoy usando pd porque anda considerablemente más rápido que np)\n",
    "train_data = pd.read_csv(r'C:/Users/kueru/Documents/VSCode/semestre_9/Deep_Learning/T2/train_data_2.csv')\n",
    "train_data = train_data.to_numpy()\n",
    "\n",
    "#Cortar en features y labales\n",
    "train_samples = train_data.shape[0]\n",
    "features = train_data[:train_samples, :-1]  # Features for training\n",
    "labels = train_data[:train_samples, -1]   # Labels for training\n",
    "\n",
    "features = cp.array(features)\n",
    "labels = cp.array(labels, ndmin=2)\n",
    "labels = labels.reshape(-1, 1)  # Reshape to (299, 1)\n",
    "\n",
    "print(train_data.shape)\n",
    "print(features.shape )\n",
    "print(labels.shape)\n",
    "\n",
    "\n",
    "indices = cp.arange(train_samples)\n",
    "\n",
    "#cortar 20% test \n",
    "test_size = 0.2\n",
    "test_samples = int(test_size * train_samples)\n",
    "train_indices, test_indices = indices[test_samples:], indices[:test_samples]\n",
    "\n",
    "X_train, X_test = features[train_indices], features[test_indices]\n",
    "y_train, y_test = labels[train_indices], labels[test_indices]\n",
    "\n",
    "train_indices = cp.array(train_indices)\n",
    "test_indices = cp.array(test_indices)\n",
    "X_train = cp.array(X_train)\n",
    "X_test = cp.array(X_test)\n",
    "y_train = cp.array(y_train)\n",
    "y_test = cp.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Obtener datos (estoy usando pd porque anda considerablemente más rápido que np)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mC:/Users/kueru/Documents/VSCode/semestre_9/Deep_Learning/T2/train_data.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m train_data \u001b[38;5;241m=\u001b[39m train_data\u001b[38;5;241m.\u001b[39mto_numpy()\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#Cortar en features y labales\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kueru\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kueru\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:625\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m--> 625\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mparser\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    626\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mreturn\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kueru\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1996\u001b[0m, in \u001b[0;36mTextFileReader.__exit__\u001b[1;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[0;32m   1990\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\n\u001b[0;32m   1991\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1992\u001b[0m     exc_type: \u001b[38;5;28mtype\u001b[39m[\u001b[38;5;167;01mBaseException\u001b[39;00m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1993\u001b[0m     exc_value: \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1994\u001b[0m     traceback: TracebackType \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1995\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1996\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kueru\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1622\u001b[0m, in \u001b[0;36mTextFileReader.close\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1619\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1620\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n\u001b[1;32m-> 1622\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclose\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1623\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1624\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Obtener datos (estoy usando pd porque anda considerablemente más rápido que np)\n",
    "train_data = pd.read_csv(r'C:/Users/kueru/Documents/VSCode/semestre_9/Deep_Learning/T2/train_data.csv')\n",
    "train_data = train_data.to_numpy()\n",
    "    \n",
    "#Cortar en features y labales\n",
    "train_samples = train_data.shape[0]\n",
    "features = train_data[:train_samples, 1:-1]  # Features for training\n",
    "labels = train_data[:train_samples, -1]   # Labels for training\n",
    "\n",
    "# Reduce pixel values\n",
    "features = features / 255.0 \n",
    "# flatten the label values\n",
    "labels = labels.flatten()\n",
    "\n",
    "features = cp.array(features)\n",
    "labels = cp.array(labels, ndmin=2)\n",
    "labels = labels.reshape(-1, 1)  # Reshape to (299, 1)\n",
    "\n",
    "print(train_data.shape)\n",
    "print(features.shape )\n",
    "print(labels.shape)\n",
    "\n",
    "\n",
    "indices = cp.arange(train_samples)\n",
    "\n",
    "\n",
    "X_train = cp.array(features)\n",
    "\n",
    "num_classes = 10\n",
    "input_size = 3072\n",
    "hidden_layers = [100, 100]  # Tamaños de las capas ocultas\n",
    "output_size = 10\n",
    "layer_sizes = [input_size] + hidden_layers + [output_size]\n",
    "\n",
    "model = MultiLayerNetwork(layer_sizes)\n",
    "epochs = 1\n",
    "learning_rate = 0.001\n",
    "\n",
    "errors = model.train(X_train, y_train, epochs, learning_rate, 2.5)\n",
    "print(errors)\n",
    "#Evaluar\n",
    "predictions = model.predict(X_train)\n",
    "accuracy = cp.mean(predictions.argmax(axis=1) == y_train)\n",
    "print(f'Test Accuracy: {accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "c261aea317cc0286b3b3261fbba9abdec21eaa57589985bb7a274bf54d6cc0a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
